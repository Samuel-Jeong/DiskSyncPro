#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
disk_sync_pro.py

CCC + SuperDuper ìŠ¤íƒ€ì¼ì„ ì°¸ê³ í•œ, ë¹„êµì  ì•ˆì „í•œ ë””ìŠ¤í¬ ë°±ì—…/ë™ê¸°í™” ìŠ¤í¬ë¦½íŠ¸.

í¬í•¨ ê¸°ëŠ¥:
- JSON ì„¤ì • íŒŒì¼ ê¸°ë°˜ ë‹¤ì¤‘ Job ê´€ë¦¬
- ëª¨ë“œ:
  - clone      : ëŒ€ìƒ í´ë”ë¥¼ ì†ŒìŠ¤ì™€ ë™ì¼í•˜ê²Œ ë¯¸ëŸ¬ë§ (ë¶ˆí•„ìš”í•œ íŒŒì¼ ì‚­ì œ)
  - sync       : ì¶”ê°€/ë³€ê²½ë§Œ ë°˜ì˜ (ì‚­ì œëŠ” í•˜ì§€ ì•ŠìŒ)
  - safety_net : ì‚­ì œ/ë®ì–´ì“°ê¸° íŒŒì¼ì„ .SafetyNet/YYYY-MM-DD/ ë¡œ ì´ë™
- ë³€ê²½ëœ íŒŒì¼ë§Œ ë³µì‚¬ (Smart Update ëŠë‚Œ)
- í•´ì‹œ ê²€ì¦ ì˜µì…˜ (verify)
- dry-run ì§€ì›
- ì›ìì  ë³µì‚¬ (ì„ì‹œ íŒŒì¼ â†’ os.replace)
- ë¡¤ë°± ì €ë„:
  - Job ì‹¤í–‰ ë™ì•ˆ ì´ë£¨ì–´ì§„ ë³€ê²½ì„ ëª¨ë‘ ê¸°ë¡
  - ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ ë¡¤ë°± ì‹œë„
  - ë‚˜ì¤‘ì— --rollback ìœ¼ë¡œ ìˆ˜ë™ ë¡¤ë°± ê°€ëŠ¥
- ë³µì‚¬ ì‹¤íŒ¨ íŒŒì¼ì€ ìŠ¤í‚µ (ì „ì²´ Jobì€ ê³„ì† ì§„í–‰)
- ìŠ¤ëƒ…ìƒ· ì¸ë±ì‹± (snapshot + index.json)
- resume checkpoint (--resume ì‚¬ìš© ì‹œ, ì¤‘ë‹¨ëœ ì§€ì  ì´í›„ë¶€í„° ì´ì–´ì„œ ì‹¤í–‰)
- ë³€ê²½ ìš”ì•½ ë¦¬í¬íŠ¸(summary_*.json) ìƒì„±
- ì§„í–‰ë¥  ë¡œê·¸ (0~100%) ì§€ì›
- ë©€í‹°ìŠ¤ë ˆë“œ íŒŒì¼ ë³µì‚¬ (Queue + Worker Threads)
- curses ê¸°ë°˜ TUI:
  - ìƒë‹¨: Job ë©”íƒ€ ì •ë³´
  - ì¤‘ë‹¨: ì§„í–‰ë¥  ë°” + í¼ì„¼íŠ¸
  - í•˜ë‹¨: ë¡œê·¸ ìŠ¤íŠ¸ë¦¼ (top ìŠ¤íƒ€ì¼)
  - ë°±ì—… ì¤‘ q í‚¤ë¡œ ì·¨ì†Œ ê°€ëŠ¥
- logs í´ë” ê¸°ì¤€ ë©”ì¸ ë©”ë‰´:
  - config ì„ íƒ â†’ ë°±ì—… ì‹¤í–‰
  - ìµœì‹  ì €ë„ ë¡¤ë°±
  - journal ëª©ë¡ ë³´ê¸°
  - snapshot ëª©ë¡ ë³´ê¸°

ì‚¬ìš© ë°©ë²•:
- ê·¸ëƒ¥ ë©”ë‰´ë¡œ ì“¸ ë•Œ:   python3 disk_sync_pro.py
- ê¸°ì¡´ CLI ê·¸ëŒ€ë¡œ:     python3 disk_sync_pro.py backup -c config.json ...
"""

import argparse
import hashlib
import json
import logging
import os
import shutil
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Optional
from threading import Thread, Lock, Event
from queue import Queue, Empty
from types import SimpleNamespace

import locale

locale.setlocale(locale.LC_ALL, "")

try:
    import curses
except ImportError:
    curses = None  # ìœˆë„ìš°/ë¯¸ì§€ì› í™˜ê²½ ëŒ€ë¹„


# ================ ì „ì—­ ì„¤ì • =================

MAX_COPY_RETRY = 3
HASH_ALGO = "sha256"
QUEUE_MAXSIZE = 10000  # ì‘ì—… í ìµœëŒ€ í¬ê¸° (ë©”ëª¨ë¦¬ í­ì£¼ ë°©ì§€)


# ================ ë¡œê¹… ì„¤ì • =================

def setup_logger(log_file: Optional[Path] = None,
                 verbose: bool = True,
                 use_tui: bool = False,
                 tui_obj: "SimpleTUI" = None) -> None:
    logger = logging.getLogger("disk_sync_pro")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()

    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")

    # ì¼ë°˜ stdout ë¡œê·¸ í•¸ë“¤ëŸ¬ (TUI ì•„ë‹ ë•Œë§Œ)
    if verbose and not use_tui:
        ch = logging.StreamHandler(sys.stdout)
        ch.setFormatter(fmt)
        logger.addHandler(ch)

    # íŒŒì¼ ë¡œê·¸ í•¸ë“¤ëŸ¬
    if log_file:
        fh = logging.FileHandler(log_file, encoding="utf-8")
        fh.setFormatter(fmt)
        logger.addHandler(fh)

    # TUI ë¡œê·¸ í•¸ë“¤ëŸ¬
    if use_tui and tui_obj is not None:
        handler = CursesLogHandler(tui_obj)
        handler.setFormatter(fmt)
        logger.addHandler(handler)


logger = logging.getLogger("disk_sync_pro")


# ================ ë°ì´í„° í´ë˜ìŠ¤ =================

@dataclass
class BackupJob:
    name: str
    source: Path
    destination: Path
    mode: str              # "clone" | "sync" | "safety_net"
    exclude: List[str]
    safety_net_days: int = 30
    verify: bool = False


@dataclass
class JournalOp:
    """
    ë¡¤ë°±ì„ ìœ„í•œ ë‹¨ì¼ ì‘ì—… ê¸°ë¡
    action:
      - create_file  : ìƒˆ íŒŒì¼ ìƒì„±
      - replace_file : ê¸°ì¡´ íŒŒì¼ ë°±ì—… í›„ ìƒˆ íŒŒì¼ë¡œ êµì²´
      - delete_file  : ê¸°ì¡´ íŒŒì¼ ì‚­ì œ(ë˜ëŠ” ë°±ì—… ìœ„ì¹˜ë¡œ ì´ë™)
      - create_dir   : ìƒˆ ë””ë ‰í† ë¦¬ ìƒì„±
    target: ìµœì¢… ëŒ€ìƒ ê²½ë¡œ
    backup: ë°±ì—…ìš©ìœ¼ë¡œ ì˜®ê²¨ë‘” ê²½ë¡œ (ì—†ëŠ” ê²½ìš° None)
    """
    action: str
    target: str
    backup: Optional[str] = None


@dataclass
class Journal:
    job_name: str
    timestamp: str
    dest_root: str
    rollback_root: str
    status: str                    # "pending" | "success" | "cancelled" | "rolled_back" | "rollback_failed"
    ops: List[JournalOp]


@dataclass
class Stats:
    created_files: int = 0
    replaced_files: int = 0
    deleted_files: int = 0
    safetynet_files: int = 0
    created_dirs: int = 0
    skipped_same: int = 0
    skipped_excluded: int = 0
    copy_failed: int = 0


@dataclass
class StageProgress:
    """
    AWS Step Function ìŠ¤íƒ€ì¼ Stage ì§„í–‰ ìƒíƒœ
    """
    stage_name: str
    status: str  # "pending" | "running" | "completed" | "failed"
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    items_total: int = 0
    items_processed: int = 0
    error: Optional[str] = None
    
    def to_dict(self):
        return {
            "stage": self.stage_name,
            "status": self.status,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "progress": f"{self.items_processed}/{self.items_total}",
            "error": self.error
        }


# ================ TUI êµ¬í˜„ (Professional Version) =================

class SimpleTUI:
    """
    ì „ë¬¸ê°€ê¸‰ TUI êµ¬í˜„:
    - í—¤ë”: Job ì •ë³´
    - ì§„í–‰ë¥  ë°”
    - ë¡œê·¸ ì˜ì—­ (ìŠ¤í¬ë¡¤ ê°€ëŠ¥)
    - ìƒíƒœë°”: í‚¤ ê°€ì´ë“œ
    - ìƒ‰ìƒ ì§€ì›
    - í™”ë©´ í¬ê¸° ë³€ê²½ ìë™ ëŒ€ì‘
    - ì•ˆì „í•œ ìœ ë‹ˆì½”ë“œ/í•œê¸€ ì²˜ë¦¬
    """
    
    # ë ˆì´ì•„ì›ƒ ìƒìˆ˜
    HEADER_HEIGHT = 5
    PROGRESS_HEIGHT = 2
    STATUSBAR_HEIGHT = 1
    LOG_SEPARATOR_HEIGHT = 1
    
    def __init__(self, stdscr):
        self.stdscr = stdscr
        self.lock = Lock()
        self.log_lines: List[str] = []
        self.job_meta = {}
        self.progress = {
            "percent": 0,
            "current": 0,
            "total": 0,
        }
        self.dirty = True
        self.log_scroll_offset = 0  # ë¡œê·¸ ìŠ¤í¬ë¡¤ ìœ„ì¹˜
        self.use_colors = False
        
        self._init_screen()
        self._init_colors()

    def _init_screen(self):
        """í™”ë©´ ì´ˆê¸°í™”"""
        try:
            self.stdscr.clear()
            self.stdscr.nodelay(True)  # non-blocking ì…ë ¥
            self.stdscr.keypad(True)
            curses.curs_set(0)  # ì»¤ì„œ ìˆ¨ê¹€
        except Exception:
            pass

    def _init_colors(self):
        """ìƒ‰ìƒ ì´ˆê¸°í™”"""
        try:
            if curses.has_colors():
                curses.start_color()
                curses.use_default_colors()
                
                # ìƒ‰ìƒ í˜ì–´ ì •ì˜
                curses.init_pair(1, curses.COLOR_CYAN, -1)     # ì œëª©
                curses.init_pair(2, curses.COLOR_GREEN, -1)    # ì§„í–‰ë¥ 
                curses.init_pair(3, curses.COLOR_YELLOW, -1)   # ê²½ê³ 
                curses.init_pair(4, curses.COLOR_RED, -1)      # ì—ëŸ¬
                curses.init_pair(5, curses.COLOR_WHITE, curses.COLOR_BLUE)  # ìƒíƒœë°”
                
                self.use_colors = True
        except Exception:
            self.use_colors = False

    def _get_screen_size(self):
        """ì•ˆì „í•œ í™”ë©´ í¬ê¸° ê°€ì ¸ì˜¤ê¸°"""
        try:
            rows, cols = self.stdscr.getmaxyx()
            return max(10, rows), max(40, cols)
        except Exception:
            return 24, 80

    def _safe_addstr(self, row: int, col: int, text: str, attr=0):
        """ì•ˆì „í•œ ë¬¸ìì—´ ì¶œë ¥ - ìœ ë‹ˆì½”ë“œ/í•œê¸€ ì§€ì›"""
        try:
            rows, cols = self._get_screen_size()
            if row >= rows or col >= cols:
                return
            
            # ë¬¸ìì—´ì„ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ì•ˆì „í•˜ê²Œ ì¶œë ¥
            available_width = cols - col - 1
            if available_width <= 0:
                return
            
            # í•œê¸€ ë“± ë©€í‹°ë°”ì´íŠ¸ ë¬¸ì ê³ ë ¤
            safe_text = text[:available_width]
            
            # ê¸¸ì´ê°€ ë„˜ìœ¼ë©´ ì ì§„ì ìœ¼ë¡œ ì¤„ì„
            while len(safe_text) > 0:
                try:
                    self.stdscr.addstr(row, col, safe_text, attr)
                    break
                except Exception:
                    safe_text = safe_text[:-1]
        except Exception:
            pass

    def _draw_separator(self, row: int, char: str = "â”€"):
        """êµ¬ë¶„ì„  ê·¸ë¦¬ê¸°"""
        try:
            rows, cols = self._get_screen_size()
            if row >= rows:
                return
            self.stdscr.move(row, 0)
            self.stdscr.clrtoeol()
            self._safe_addstr(row, 0, char * cols)
        except Exception:
            pass

    # ===== ìƒíƒœ ì—…ë°ì´íŠ¸ (ë©€í‹°ìŠ¤ë ˆë“œ ì•ˆì „) =====
    def set_job_meta(self, **meta):
        with self.lock:
            self.job_meta = meta
            self.dirty = True

    def update_progress(self, percent: int, current: int, total: int):
        with self.lock:
            self.progress["percent"] = min(100, max(0, percent))
            self.progress["current"] = current
            self.progress["total"] = total
            self.dirty = True

    def add_log_line(self, text: str):
        with self.lock:
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°í•˜ê³  ë©”ì‹œì§€ë§Œ ì €ì¥ (ê¹”ë”í•œ í‘œì‹œ)
            msg = text
            if '] ' in text:
                parts = text.split('] ', 1)
                if len(parts) == 2:
                    msg = parts[1]
            
            self.log_lines.append(msg)
            
            # ë©”ëª¨ë¦¬ ê´€ë¦¬
            if len(self.log_lines) > 5000:
                self.log_lines = self.log_lines[-2000:]
            
            self.dirty = True

    # ===== í™”ë©´ ê·¸ë¦¬ê¸° (ë©”ì¸ ìŠ¤ë ˆë“œ ì „ìš©) =====
    def refresh_if_dirty(self):
        """ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ í™”ë©´ ê°±ì‹ """
        with self.lock:
            if not self.dirty:
                return
            
            try:
                self._draw_all()
                self.stdscr.refresh()
                self.dirty = False
            except Exception:
                # í™”ë©´ í¬ê¸° ë³€ê²½ ë“±ì˜ ì´ìœ ë¡œ ì‹¤íŒ¨í•˜ë©´ ì¬ì´ˆê¸°í™” ì‹œë„
                try:
                    self.stdscr.clear()
                    self._draw_all()
                    self.stdscr.refresh()
                    self.dirty = False
                except Exception:
                    pass

    def _draw_all(self):
        """ì „ì²´ í™”ë©´ ê·¸ë¦¬ê¸°"""
        rows, cols = self._get_screen_size()
        
        # í™”ë©´ ì´ˆê¸°í™”
        try:
            self.stdscr.erase()
        except Exception:
            pass
        
        # 1. í—¤ë” (Job ì •ë³´)
        self._draw_header()
        
        # 2. êµ¬ë¶„ì„ 
        self._draw_separator(self.HEADER_HEIGHT)
        
        # 3. ì§„í–‰ë¥ 
        progress_start = self.HEADER_HEIGHT + 1
        self._draw_progress(progress_start)
        
        # 4. ë¡œê·¸ ì˜ì—­
        log_start = progress_start + self.PROGRESS_HEIGHT + self.LOG_SEPARATOR_HEIGHT
        self._draw_separator(log_start - 1)
        self._draw_logs(log_start)
        
        # 5. ìƒíƒœë°”
        self._draw_statusbar()

    def _draw_header(self):
        """í—¤ë” ì˜ì—­ ê·¸ë¦¬ê¸°"""
        rows, cols = self._get_screen_size()
        
        # ì œëª©
        title = "DiskSyncPro - Professional Backup System"
        title_attr = curses.color_pair(1) | curses.A_BOLD if self.use_colors else curses.A_BOLD
        self._safe_addstr(0, (cols - len(title)) // 2, title, title_attr)
        
        # Job ì •ë³´
        job_name = self.job_meta.get('job_name', 'N/A')
        source = self.job_meta.get('source', 'N/A')
        destination = self.job_meta.get('destination', 'N/A')
        mode = self.job_meta.get('mode', 'N/A')
        
        self._safe_addstr(1, 2, f"Job: {job_name}")
        self._safe_addstr(2, 2, f"Src: {source}")
        self._safe_addstr(3, 2, f"Dst: {destination}")
        
        # ì˜µì…˜ ì •ë³´
        verify = "ON" if self.job_meta.get('verify', False) else "OFF"
        threads = self.job_meta.get('threads', 'N/A')
        resume = "ON" if self.job_meta.get('resume', False) else "OFF"
        
        info = f"Mode: {mode} | Verify: {verify} | Threads: {threads} | Resume: {resume}"
        self._safe_addstr(4, 2, info)

    def _draw_progress(self, start_row: int):
        """ì§„í–‰ë¥  ë°” ê·¸ë¦¬ê¸°"""
        rows, cols = self._get_screen_size()
        
        percent = self.progress["percent"]
        current = self.progress["current"]
        total = self.progress["total"]
        
        # ì§„í–‰ë¥  ë°”
        bar_width = max(20, cols - 30)
        filled = int(bar_width * percent / 100)
        bar = "â–ˆ" * filled + "â–‘" * (bar_width - filled)
        
        bar_attr = curses.color_pair(2) if self.use_colors else 0
        self._safe_addstr(start_row, 2, f"Progress: [{bar}] {percent}%", bar_attr)
        
        # íŒŒì¼ ìˆ˜ ì •ë³´
        if total > 0:
            speed_info = f"Files: {current:,} / {total:,}"
        else:
            speed_info = f"Files: {current:,}"
        
        self._safe_addstr(start_row + 1, 2, speed_info)

    def _draw_logs(self, start_row: int):
        """ë¡œê·¸ ì˜ì—­ ê·¸ë¦¬ê¸°"""
        rows, cols = self._get_screen_size()
        
        # ë¡œê·¸ ì˜ì—­ ë†’ì´ ê³„ì‚°
        log_height = rows - start_row - self.STATUSBAR_HEIGHT
        if log_height <= 0:
            return
        
        # í‘œì‹œí•  ë¡œê·¸ ë¼ì¸ ì„ íƒ
        total_logs = len(self.log_lines)
        if total_logs == 0:
            self._safe_addstr(start_row, 2, "Waiting for log messages...")
            return
        
        # ìŠ¤í¬ë¡¤ ì˜¤í”„ì…‹ ì¡°ì •
        max_offset = max(0, total_logs - log_height)
        self.log_scroll_offset = min(self.log_scroll_offset, max_offset)
        
        # ìµœì‹  ë¡œê·¸ê°€ í•­ìƒ ë³´ì´ë„ë¡ (ìë™ ìŠ¤í¬ë¡¤)
        if self.log_scroll_offset == 0 or total_logs <= log_height:
            # ìµœì‹  ë¡œê·¸ í‘œì‹œ
            start_idx = max(0, total_logs - log_height)
            lines_to_show = self.log_lines[start_idx:]
        else:
            # ìŠ¤í¬ë¡¤ëœ ìœ„ì¹˜ì˜ ë¡œê·¸ í‘œì‹œ
            lines_to_show = self.log_lines[self.log_scroll_offset:self.log_scroll_offset + log_height]
        
        # ë¡œê·¸ ì¶œë ¥
        for idx, line in enumerate(lines_to_show):
            row = start_row + idx
            if row >= rows - self.STATUSBAR_HEIGHT:
                break
            
            # ë¡œê·¸ ë ˆë²¨ì— ë”°ë¥¸ ìƒ‰ìƒ ì ìš©
            attr = 0
            if self.use_colors:
                if 'ERROR' in line or 'FAIL' in line or 'ì‹¤íŒ¨' in line:
                    attr = curses.color_pair(4)
                elif 'WARN' in line or 'ê²½ê³ ' in line:
                    attr = curses.color_pair(3)
                elif 'SUCCESS' in line or 'ì„±ê³µ' in line or 'ì™„ë£Œ' in line:
                    attr = curses.color_pair(2)
            
            self._safe_addstr(row, 2, line, attr)

    def _draw_statusbar(self):
        """í•˜ë‹¨ ìƒíƒœë°” ê·¸ë¦¬ê¸°"""
        rows, cols = self._get_screen_size()
        statusbar_row = rows - 1
        
        status_text = " [Q] Cancel  [â†‘â†“] Scroll Logs  [R] Refresh "
        status_attr = curses.color_pair(5) if self.use_colors else curses.A_REVERSE
        
        # ìƒíƒœë°” ì „ì²´ë¥¼ ìƒ‰ìƒìœ¼ë¡œ ì±„ìš°ê¸°
        try:
            self.stdscr.move(statusbar_row, 0)
            self.stdscr.clrtoeol()
            
            # ê°€ìš´ë° ì •ë ¬
            padding = (cols - len(status_text)) // 2
            full_text = " " * padding + status_text + " " * (cols - padding - len(status_text))
            self._safe_addstr(statusbar_row, 0, full_text[:cols], status_attr)
        except Exception:
            pass

    def check_cancel_key(self) -> bool:
        """í‚¤ ì…ë ¥ ì²´í¬ (ì·¨ì†Œ, ìŠ¤í¬ë¡¤ ë“±)"""
        try:
            ch = self.stdscr.getch()
            if ch == -1:
                return False
            
            if ch in (ord('q'), ord('Q')):
                self.add_log_line("[USER] ì·¨ì†Œ ìš”ì²­ - ë°±ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤...")
                return True
            elif ch == curses.KEY_UP:
                # ë¡œê·¸ ìŠ¤í¬ë¡¤ ì—…
                with self.lock:
                    self.log_scroll_offset = max(0, self.log_scroll_offset - 1)
                    self.dirty = True
            elif ch == curses.KEY_DOWN:
                # ë¡œê·¸ ìŠ¤í¬ë¡¤ ë‹¤ìš´
                with self.lock:
                    rows, cols = self._get_screen_size()
                    log_height = rows - (self.HEADER_HEIGHT + 1 + self.PROGRESS_HEIGHT + self.LOG_SEPARATOR_HEIGHT + self.STATUSBAR_HEIGHT)
                    max_offset = max(0, len(self.log_lines) - log_height)
                    self.log_scroll_offset = min(self.log_scroll_offset + 1, max_offset)
                    self.dirty = True
            elif ch in (ord('r'), ord('R')):
                # ê°•ì œ ìƒˆë¡œê³ ì¹¨
                with self.lock:
                    self.dirty = True
            
            return False
        except Exception:
            return False

    def redraw_all(self):
        """ì „ì²´ í™”ë©´ ê°•ì œ ë‹¤ì‹œ ê·¸ë¦¬ê¸°"""
        with self.lock:
            self.dirty = True
        self.refresh_if_dirty()


class CursesLogHandler(logging.Handler):
    """
    logging â†’ TUI ë¡œê·¸ì°½ìœ¼ë¡œ ë³´ë‚´ëŠ” í•¸ë“¤ëŸ¬
    (ì—¬ê¸°ì„œëŠ” curses í˜¸ì¶œ ì—†ì´ ìƒíƒœë§Œ ì—…ë°ì´íŠ¸)
    """
    def __init__(self, tui: SimpleTUI):
        super().__init__()
        self.tui = tui

    def emit(self, record):
        try:
            msg = self.format(record)
            self.tui.add_log_line(msg)
        except Exception:
            pass


# ================ ì„¤ì • ë¡œë”© =================

def load_config(config_path: Path) -> List[BackupJob]:
    """
    JSON ì„¤ì • íŒŒì¼ì„ ì½ì–´ BackupJob ë¦¬ìŠ¤íŠ¸ ìƒì„±
    """
    with config_path.open("r", encoding="utf-8") as f:
        raw = json.load(f)

    jobs: List[BackupJob] = []
    for job in raw.get("jobs", []):
        jobs.append(
            BackupJob(
                name=job["name"],
                source=Path(job["source"]).expanduser(),
                destination=Path(job["destination"]).expanduser(),
                mode=job.get("mode", "safety_net"),
                exclude=job.get("exclude", []),
                safety_net_days=job.get("safety_net_days", 30),
                verify=job.get("verify", False),
            )
        )
    return jobs


# ================ ìœ í‹¸ =================

def path_matches_patterns(path: Path, patterns: List[str]) -> bool:
    """
    ì œì™¸ íŒ¨í„´ ì²˜ë¦¬:
    - ë‹¨ìˆœ ì´ë¦„
    - glob íŒ¨í„´ (*.tmp, *.log ë“±)
    """
    name = path.name
    for pattern in patterns:
        if pattern == name:
            return True
        if path.match(pattern) or name == pattern:
            return True
    return False


def file_hash(path: Path, algo: str = HASH_ALGO, chunk_size: int = 1024 * 1024) -> str:
    """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ê²€ì¦ìš©)"""
    h = hashlib.new(algo)
    with path.open("rb") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def is_same_file(src: Path, dst: Path) -> bool:
    """
    ì„±ëŠ¥ ìš°ì„ : íŒŒì¼ í¬ê¸° + mtime ìœ¼ë¡œ ë™ì¼ ì—¬ë¶€ íŒë‹¨
    """
    if not dst.exists():
        return False
    s_stat = src.stat()
    d_stat = dst.stat()
    return (s_stat.st_size == d_stat.st_size) and (int(s_stat.st_mtime) == int(d_stat.st_mtime))


def atomic_copy(src: Path, dst: Path) -> None:
    """
    ì„ì‹œ íŒŒì¼ì— ë³µì‚¬ í›„ os.replace ë¡œ êµì²´í•˜ëŠ” ì›ìì (atomic) ë³µì‚¬.
    """
    dst_parent = dst.parent
    dst_parent.mkdir(parents=True, exist_ok=True)
    tmp_name = f".{dst.name}.sbk_tmp_{os.getpid()}"
    tmp_path = dst_parent / tmp_name

    try:
        if tmp_path.exists():
            tmp_path.unlink()
    except Exception:
        pass

    shutil.copy2(src, tmp_path)
    os.replace(tmp_path, dst)


def ensure_dir(path: Path, journal: Optional[Journal] = None,
               stats: Optional[Stats] = None, dry_run: bool = False) -> None:
    """
    ë””ë ‰í† ë¦¬ ìƒì„±. ë¡¤ë°±ì„ ìœ„í•´ create_dir ê¸°ë¡.
    """
    if path.exists():
        return
    logger.info(f"[MKDIR] {path}")
    if dry_run:
        return
    path.mkdir(parents=True, exist_ok=True)
    if journal:
        journal.ops.append(JournalOp(action="create_dir", target=str(path)))
    if stats:
        stats.created_dirs += 1


# ================ SafetyNet / Rollback ì˜ì—­ =================

def get_safety_net_dir(destination_root: Path) -> Path:
    today = datetime.now().strftime("%Y-%m-%d")
    sn_root = destination_root / ".SafetyNet" / today
    sn_root.mkdir(parents=True, exist_ok=True)
    return sn_root


def move_to_safety_net(target: Path, dest_root: Path, dry_run: bool = False) -> Path:
    """
    ì‚­ì œ/ë®ì–´ì“°ê¸° ëŒ€ìƒ íŒŒì¼ì„ SafetyNetìœ¼ë¡œ ì´ë™
    """
    sn_root = get_safety_net_dir(dest_root)
    try:
        rel = target.relative_to(dest_root)
    except ValueError:
        rel = Path(target.name)

    sn_path = sn_root / rel
    logger.info(f"[SafetyNet] {target} -> {sn_path}")
    if not dry_run:
        sn_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.move(str(target), str(sn_path))
    return sn_path


def prepare_journal(job: BackupJob) -> Journal:
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    rollback_root = job.destination / ".Rollback" / f"{job.name}_{ts}"
    rollback_root.mkdir(parents=True, exist_ok=True)
    return Journal(
        job_name=job.name,
        timestamp=ts,
        dest_root=str(job.destination),
        rollback_root=str(rollback_root),
        status="pending",
        ops=[],
    )


def journal_path_for(job: BackupJob, log_dir: Path, ts: str) -> Path:
    return log_dir / f"journal_{job.name}_{ts}.json"


def get_dest_meta_dir(destination_root: Path) -> Path:
    """íƒ€ê²Ÿ ë””ìŠ¤í¬ì˜ ë©”íƒ€ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬"""
    meta_dir = destination_root / ".DiskSyncPro"
    meta_dir.mkdir(parents=True, exist_ok=True)
    return meta_dir


def save_journal(journal: Journal, path: Path, destination_root: Optional[Path] = None) -> None:
    """ì €ë„ì„ logs í´ë”ì™€ íƒ€ê²Ÿ í´ë” ëª¨ë‘ì— ì €ì¥"""
    serializable = {
        "job_name": journal.job_name,
        "timestamp": journal.timestamp,
        "dest_root": journal.dest_root,
        "rollback_root": journal.rollback_root,
        "status": journal.status,
        "ops": [asdict(op) for op in journal.ops],
    }
    # logs í´ë”ì— ì €ì¥
    with path.open("w", encoding="utf-8") as f:
        json.dump(serializable, f, indent=2, ensure_ascii=False)
    
    # íƒ€ê²Ÿ í´ë”ì—ë„ ì €ì¥
    if destination_root and destination_root.exists():
        try:
            dest_meta_dir = get_dest_meta_dir(destination_root)
            dest_journal_path = dest_meta_dir / path.name
            with dest_journal_path.open("w", encoding="utf-8") as f:
                json.dump(serializable, f, indent=2, ensure_ascii=False)
            logger.info(f"ì €ë„ ë³µì‚¬ë³¸ ì €ì¥: {dest_journal_path}")
        except Exception as e:
            logger.warning(f"íƒ€ê²Ÿ í´ë” ì €ë„ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_journal(path: Path) -> Journal:
    with path.open("r", encoding="utf-8") as f:
        raw = json.load(f)
    ops = [JournalOp(**op) for op in raw.get("ops", [])]
    return Journal(
        job_name=raw["job_name"],
        timestamp=raw["timestamp"],
        dest_root=raw["dest_root"],
        rollback_root=raw["rollback_root"],
        status=raw.get("status", "pending"),
        ops=ops,
    )


# ================ Checkpoint (resume) =================

def load_or_init_checkpoint(job: BackupJob, log_dir: Path) -> dict:
    """
    checkpoint_<job>.json íŒŒì¼ì„ ì½ì–´ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„±.
    status != 'incomplete' ì¸ ê²½ìš° processed ëŠ” ì´ˆê¸°í™”.
    
    ê°œì„ : ë””ë ‰í† ë¦¬ ë‹¨ìœ„ ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
    """
    path = log_dir / f"checkpoint_{job.name}.json"
    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        status = data.get("status", "incomplete")
        if status == "incomplete":
            processed = set(data.get("processed_files", []))
            processed_dirs = set(data.get("processed_dirs", []))  # ì™„ë£Œëœ ë””ë ‰í† ë¦¬
        else:
            processed = set()
            processed_dirs = set()
    else:
        status = "incomplete"
        processed = set()
        processed_dirs = set()
    
    cp = {
        "job_name": job.name,
        "status": status,
        "processed": processed,
        "processed_dirs": processed_dirs,  # ë””ë ‰í† ë¦¬ ë‹¨ìœ„ ì¶”ì 
        "path": path,
    }
    return cp


def save_checkpoint(cp: dict) -> None:
    """
    ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ë””ë ‰í† ë¦¬ ë‹¨ìœ„ ì¶”ì  í¬í•¨)
    """
    if cp is None:
        return
    path: Path = cp["path"]
    data = {
        "job_name": cp["job_name"],
        "status": cp["status"],
        "processed_files": sorted(list(cp["processed"]))[:1000],  # ìµœê·¼ 1000ê°œë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
        "processed_dirs": sorted(list(cp.get("processed_dirs", set()))),  # ì™„ë£Œëœ ë””ë ‰í† ë¦¬
        "last_updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "total_processed": len(cp["processed"]),  # ì „ì²´ ì²˜ë¦¬ ìˆ˜
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)


# ================ ì§„í–‰ë¥  ê³„ì‚°ìš© =================

def count_total_files_for_job(job: BackupJob) -> int:
    """
    ì§„í–‰ë¥  ê³„ì‚°ì„ ìœ„í•´ ì†ŒìŠ¤ ì•„ë˜ 'ëŒ€ìƒ íŒŒì¼ ìˆ˜'ë¥¼ ë¯¸ë¦¬ ìƒ˜.
    exclude íŒ¨í„´ì— ê±¸ë¦¬ëŠ” íŒŒì¼ì€ ì œì™¸.
    """
    total = 0
    for root, dirs, files in os.walk(job.source):
        root_path = Path(root)
        dirs[:] = [d for d in dirs if not path_matches_patterns(root_path / d, job.exclude)]
        for f in files:
            p = root_path / f
            if path_matches_patterns(p, job.exclude):
                continue
            total += 1
    return total


# ================ ë¡¤ë°± =================

def rollback_journal(journal: Journal, dry_run: bool = False) -> None:
    """
    Journal ì„ ì—­ìˆœìœ¼ë¡œ ì½ì–´ ë¡¤ë°± ìˆ˜í–‰.
    """
    logger.info(f"=== ë¡¤ë°± ì‹œì‘: job={journal.job_name}, ts={journal.timestamp} ===")

    for op in reversed(journal.ops):
        target = Path(op.target)
        backup = Path(op.backup) if op.backup else None

        if op.action == "create_file":
            if target.exists():
                logger.info(f"[ROLLBACK delete created file] {target}")
                if not dry_run:
                    try:
                        target.unlink()
                    except Exception as e:
                        logger.error(f"ë¡¤ë°±: íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {target}: {e}")

        elif op.action in ("replace_file", "delete_file"):
            if backup and backup.exists():
                logger.info(f"[ROLLBACK restore] {backup} -> {target}")
                if not dry_run:
                    target.parent.mkdir(parents=True, exist_ok=True)
                    try:
                        if target.exists():
                            target.unlink()
                        shutil.move(str(backup), str(target))
                    except Exception as e:
                        logger.error(f"ë¡¤ë°±: ë³µì› ì‹¤íŒ¨ {backup} -> {target}: {e}")

        elif op.action == "create_dir":
            if target.exists() and target.is_dir():
                try:
                    target.rmdir()
                    logger.info(f"[ROLLBACK rmdir] {target}")
                except OSError:
                    pass

    logger.info("=== ë¡¤ë°± ì¢…ë£Œ ===")


# ================ í•µì‹¬ ë°±ì—… ë¡œì§ (ë©€í‹°ìŠ¤ë ˆë“œ ë³µì‚¬) =================

def copy_with_retry(src: Path,
                    dst: Path,
                    verify: bool,
                    journal: Journal,
                    stats: Stats,
                    stats_lock: Lock,
                    journal_lock: Lock,
                    dry_run: bool = False) -> bool:
    """
    ì›ìì  ë³µì‚¬ + ì¬ì‹œë„ + í•´ì‹œ ê²€ì¦ + ì €ë„ ê¸°ë¡
    ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ì˜¬ë¦¬ì§€ ì•Šê³  False ë¥¼ ë°˜í™˜í•´ì„œ
    í•´ë‹¹ íŒŒì¼ë§Œ ìŠ¤í‚µí•˜ë„ë¡ ë™ì‘.
    ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ í˜¸ì¶œë˜ë¯€ë¡œ stats/journal ì—…ë°ì´íŠ¸ëŠ” ë½ìœ¼ë¡œ ë³´í˜¸.
    """
    action = "replace_file" if dst.exists() else "create_file"
    backup_path = None

    if action == "replace_file" and not dry_run:
        backup_path = Path(journal.rollback_root) / dst.relative_to(Path(journal.dest_root))
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        logger.info(f"[BACKUP(before replace)] {dst} -> {backup_path}")
        try:
            shutil.copy2(dst, backup_path)
        except Exception as e:
            logger.error(f"[BACKUP ì‹¤íŒ¨] {dst} -> {backup_path}: {e}")

    if dry_run:
        logger.info(f"[COPY (dry-run)] {src} -> {dst}")
        with journal_lock:
            journal.ops.append(JournalOp(
                action=action,
                target=str(dst),
                backup=str(backup_path) if backup_path else None
            ))
        with stats_lock:
            if action == "create_file":
                stats.created_files += 1
            else:
                stats.replaced_files += 1
        return True
    else:
        success = False
        for attempt in range(1, MAX_COPY_RETRY + 1):
            try:
                logger.info(f"[COPY] {src} -> {dst} (attempt {attempt})")
                atomic_copy(src, dst)
                if verify:
                    src_hash = file_hash(src)
                    dst_hash = file_hash(dst)
                    if src_hash != dst_hash:
                        raise IOError(f"í•´ì‹œ ë¶ˆì¼ì¹˜: {src} != {dst}")
                success = True
                break
            except Exception as e:
                logger.error(f"ë³µì‚¬ ì‹¤íŒ¨ ({attempt}/{MAX_COPY_RETRY}): {src} -> {dst}: {e}")

        if not success:
            logger.error(f"[SKIP] ìµœëŒ€ ì¬ì‹œë„ ì‹¤íŒ¨ë¡œ ì´ íŒŒì¼ì€ ìŠ¤í‚µí•©ë‹ˆë‹¤: {src}")
            with stats_lock:
                stats.copy_failed += 1
            return False

    with journal_lock:
        journal.ops.append(JournalOp(
            action=action,
            target=str(dst),
            backup=str(backup_path) if backup_path else None
        ))
    with stats_lock:
        if action == "create_file":
            stats.created_files += 1
        else:
            stats.replaced_files += 1
    return True


# ================ Snapshot & Summary =================

def build_snapshot(job: BackupJob, journal: Journal, log_dir: Path) -> Path:
    """
    ë°±ì—… ì™„ë£Œ í›„ destination ì „ì²´ ìŠ¤ëƒ…ìƒ·(manifest) ìƒì„±.
    logs í´ë”ì™€ íƒ€ê²Ÿ í´ë” ëª¨ë‘ì— ì €ì¥.
    """
    dest_root = job.destination
    snapshot_dir = log_dir / "snapshots" / job.name
    snapshot_dir.mkdir(parents=True, exist_ok=True)

    files_manifest = []

    for root, dirs, files in os.walk(dest_root):
        root_path = Path(root)
        if any(x in root_path.parts for x in (".Rollback", ".SafetyNet", ".DiskSyncPro")):
            dirs[:] = []
            continue

        for f in files:
            file_path = root_path / f
            rel_path = file_path.relative_to(dest_root).as_posix()
            st = file_path.stat()
            entry = {
                "path": rel_path,
                "size": st.st_size,
                "mtime": int(st.st_mtime),
            }
            if job.verify:
                entry["hash"] = file_hash(file_path)
            files_manifest.append(entry)

    snapshot_data = {
        "job_name": job.name,
        "timestamp": journal.timestamp,
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "mode": job.mode,
        "source": str(job.source),
        "destination": str(job.destination),
        "file_count": len(files_manifest),
        "files": files_manifest,
    }

    # logs í´ë”ì— ì €ì¥
    snapshot_file = snapshot_dir / f"snapshot_{journal.timestamp}.json"
    with snapshot_file.open("w", encoding="utf-8") as f:
        json.dump(snapshot_data, f, indent=2, ensure_ascii=False)

    index_file = snapshot_dir / "index.json"
    if index_file.exists():
        with index_file.open("r", encoding="utf-8") as f:
            index = json.load(f)
    else:
        index = []

    index.append({
        "timestamp": snapshot_data["timestamp"],
        "snapshot_file": snapshot_file.name,
        "file_count": len(files_manifest),
        "generated_at": snapshot_data["generated_at"],
    })

    with index_file.open("w", encoding="utf-8") as f:
        json.dump(index, f, indent=2, ensure_ascii=False)

    # íƒ€ê²Ÿ í´ë”ì—ë„ ì €ì¥
    if dest_root.exists():
        try:
            dest_meta_dir = get_dest_meta_dir(dest_root)
            dest_snapshot_dir = dest_meta_dir / "snapshots"
            dest_snapshot_dir.mkdir(parents=True, exist_ok=True)
            
            dest_snapshot_file = dest_snapshot_dir / f"snapshot_{journal.timestamp}.json"
            with dest_snapshot_file.open("w", encoding="utf-8") as f:
                json.dump(snapshot_data, f, indent=2, ensure_ascii=False)
            
            dest_index_file = dest_snapshot_dir / "index.json"
            if dest_index_file.exists():
                with dest_index_file.open("r", encoding="utf-8") as f:
                    dest_index = json.load(f)
            else:
                dest_index = []
            
            dest_index.append({
                "timestamp": snapshot_data["timestamp"],
                "snapshot_file": dest_snapshot_file.name,
                "file_count": len(files_manifest),
                "generated_at": snapshot_data["generated_at"],
            })
            
            with dest_index_file.open("w", encoding="utf-8") as f:
                json.dump(dest_index, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ìŠ¤ëƒ…ìƒ· ë³µì‚¬ë³¸ ì €ì¥: {dest_snapshot_file}")
        except Exception as e:
            logger.warning(f"íƒ€ê²Ÿ í´ë” ìŠ¤ëƒ…ìƒ· ì €ì¥ ì‹¤íŒ¨: {e}")

    return snapshot_file


def write_summary(job: BackupJob, journal: Journal, stats: Stats, log_dir: Path) -> Path:
    """
    ë³€ê²½ ìš”ì•½ ë¦¬í¬íŠ¸ JSON ìƒì„±.
    logs í´ë”ì™€ íƒ€ê²Ÿ í´ë” ëª¨ë‘ì— ì €ì¥.
    """
    summary = {
        "job_name": job.name,
        "timestamp": journal.timestamp,
        "mode": job.mode,
        "source": str(job.source),
        "destination": str(job.destination),
        "stats": asdict(stats),
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    # logs í´ë”ì— ì €ì¥
    summary_file = log_dir / f"summary_{job.name}_{journal.timestamp}.json"
    with summary_file.open("w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    # íƒ€ê²Ÿ í´ë”ì—ë„ ì €ì¥
    if job.destination.exists():
        try:
            dest_meta_dir = get_dest_meta_dir(job.destination)
            dest_summary_file = dest_meta_dir / f"summary_{journal.timestamp}.json"
            with dest_summary_file.open("w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            logger.info(f"ìš”ì•½ ë³µì‚¬ë³¸ ì €ì¥: {dest_summary_file}")
        except Exception as e:
            logger.warning(f"íƒ€ê²Ÿ í´ë” ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")

    logger.info("=== ë³€ê²½ ìš”ì•½ ===")
    for k, v in summary["stats"].items():
        logger.info(f"{k}: {v}")
    logger.info("================")

    return summary_file


# ================ Backup ì‹¤í–‰ (ë©€í‹°ìŠ¤ë ˆë“œ + TUI) =================

def perform_backup(job: BackupJob,
                   dry_run: bool,
                   log_dir: Path,
                   resume: bool,
                   tui: Optional[SimpleTUI] = None) -> None:
    logger.info(f"=== Job ì‹œì‘: {job.name} ===")
    logger.info(f"  Source      : {job.source}")
    logger.info(f"  Destination : {job.destination}")
    logger.info(f"  Mode        : {job.mode}")
    logger.info(f"  Exclude     : {job.exclude}")
    logger.info(f"  Verify      : {job.verify}")
    logger.info(f"  Dry-run     : {dry_run}")
    logger.info(f"  Resume      : {resume}")

    if not job.source.exists():
        logger.error(f"ì†ŒìŠ¤ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {job.source}")
        return

    if job.mode not in ("clone", "sync", "safety_net"):
        logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë“œì…ë‹ˆë‹¤: {job.mode}")
        return

    if not dry_run:
        job.destination.mkdir(parents=True, exist_ok=True)

    # ë©€í‹°ìŠ¤ë ˆë“œ ì„¤ì • (I/O bound ì‘ì—…ì´ë¯€ë¡œ ë” ë§ì€ ìŠ¤ë ˆë“œ ì‚¬ìš©)
    cpu_count = os.cpu_count() or 4
    if total_files > 100000:
        # ëŒ€ëŸ‰ íŒŒì¼: CPU * 4
        num_threads = min(64, max(8, cpu_count * 4))
    elif total_files > 10000:
        # ì¤‘ê°„ íŒŒì¼: CPU * 3
        num_threads = min(48, max(6, cpu_count * 3))
    else:
        # ì†ŒëŸ‰ íŒŒì¼: CPU * 2
        num_threads = min(32, max(4, cpu_count * 2))
    
    logger.info(f"ğŸ“Š ë©€í‹°ìŠ¤ë ˆë“œ ì„¤ì •: CPU={cpu_count}ì½”ì–´, íŒŒì¼={total_files:,}ê°œ â†’ ìŠ¤ë ˆë“œ={num_threads}ê°œ")

    if tui is not None:
        tui.set_job_meta(
            job_name=job.name,
            source=str(job.source),
            destination=str(job.destination),
            mode=job.mode,
            verify=job.verify,
            threads=num_threads,
            resume=resume,
        )
        tui.refresh_if_dirty()

    cancel_event = Event()
    cancelled = False

    total_files = count_total_files_for_job(job)
    if total_files == 0:
        logger.info("ì²˜ë¦¬í•  ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (0ê°œ)")
    else:
        logger.info(f"ì´ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ìˆ˜: {total_files}")

    journal = prepare_journal(job)
    journal_file = journal_path_for(job, log_dir, journal.timestamp)
    logger.info(f"ì €ë„ íŒŒì¼: {journal_file}")
    save_journal(journal, journal_file, destination_root=job.destination)

    stats = Stats()

    cp = None
    already_processed = 0
    cp_lock = Lock()
    if resume and not dry_run:
        cp = load_or_init_checkpoint(job, log_dir)
        cp["status"] = "incomplete"
        already_processed = len(cp["processed"])
        save_checkpoint(cp)
        if total_files > 0 and already_processed > 0:
            logger.info("=" * 60)
            logger.info(f"ğŸ”„ RESUME ëª¨ë“œ í™œì„±í™”")
            logger.info(f"   ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼: {already_processed:,}ê°œ")
            logger.info(f"   ì²˜ë¦¬í•  íŒŒì¼: {total_files - already_processed:,}ê°œ")
            logger.info(f"   ì§„í–‰ë¥ : {int(already_processed * 100 / total_files)}%")
            logger.info("=" * 60)

    progress_lock = Lock()
    if total_files > 0:
        if resume and already_processed > 0:
            current_processed = min(already_processed, total_files)
            last_percent = int(current_processed * 100 / total_files)
            logger.info(
                f"[PROGRESS] {job.name}: {last_percent}% "
                f"(resume: {current_processed}/{total_files})"
            )
        else:
            current_processed = 0
            last_percent = 0
            logger.info(f"[PROGRESS] {job.name}: 0% (0/{total_files})")
    else:
        current_processed = 0
        last_percent = 100

    if tui is not None:
        if total_files > 0:
            tui.update_progress(last_percent, current_processed, total_files)
        else:
            tui.update_progress(100, 0, 0)
        tui.refresh_if_dirty()

    # ì§„í–‰ë¥  ë³´ê³  ìµœì í™”
    last_log_time = datetime.now()
    progress_log_interval = 5  # 5ì´ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥
    
    def report_progress():
        nonlocal current_processed, last_percent, last_log_time
        if total_files == 0:
            return
        with progress_lock:
            current_processed += 1
            if current_processed > total_files:
                current_processed = total_files
            percent = int(current_processed * 100 / total_files)
            
            # TUIëŠ” í•­ìƒ ì—…ë°ì´íŠ¸
            if tui is not None:
                tui.update_progress(percent, current_processed, total_files)
            
            # ë¡œê·¸ëŠ” í¼ì„¼íŠ¸ ë³€ê²½ ë˜ëŠ” 5ì´ˆ ê²½ê³¼ ì‹œë§Œ ì¶œë ¥
            now = datetime.now()
            time_elapsed = (now - last_log_time).total_seconds()
            
            if percent > last_percent or time_elapsed >= progress_log_interval:
                last_percent = percent
                last_log_time = now
                
                if not tui:
                    # íŒŒì¼ ì†ë„ ê³„ì‚°
                    if time_elapsed > 0:
                        speed = (current_processed - (current_processed - 1)) / time_elapsed
                        logger.info(
                            f"[PROGRESS] {job.name}: {percent}% "
                            f"({current_processed:,}/{total_files:,}) "
                            f"[{speed:.1f} files/s]"
                        )

    def add_processed_file_safe(rel_path: str):
        if cp is None:
            return
        with cp_lock:
            if rel_path in cp["processed"]:
                return
            cp["processed"].add(rel_path)
            save_checkpoint(cp)

    # ============ AWS Step Function ìŠ¤íƒ€ì¼ Stage ê´€ë¦¬ ============
    stages = {
        "STAGE_1_SCAN": StageProgress("íŒŒì¼ ìŠ¤ìº” ë° ê³„íš", "pending"),
        "STAGE_2_COPY": StageProgress("íŒŒì¼ ë³µì‚¬ (ë©€í‹°ìŠ¤ë ˆë“œ)", "pending"),
        "STAGE_3_CLEANUP": StageProgress("ì •ë¦¬ ë° ë™ê¸°í™”", "pending"),
        "STAGE_4_SNAPSHOT": StageProgress("ìŠ¤ëƒ…ìƒ· ìƒì„±", "pending"),
    }
    
    def update_stage(stage_key: str, status: str, **kwargs):
        """Stage ìƒíƒœ ì—…ë°ì´íŠ¸"""
        stage = stages[stage_key]
        stage.status = status
        
        if status == "running" and not stage.start_time:
            stage.start_time = datetime.now().strftime("%H:%M:%S")
        elif status in ("completed", "failed"):
            stage.end_time = datetime.now().strftime("%H:%M:%S")
        
        for key, value in kwargs.items():
            setattr(stage, key, value)
        
        # ë¡œê·¸ ì¶œë ¥
        logger.info(f"[STAGE] {stage.stage_name}: {status.upper()}")
        if tui:
            tui.add_log_line(f"[STAGE] {stage.stage_name}: {status.upper()}")
    
    # ============ ë©€í‹°ìŠ¤ë ˆë“œ Worker ì„¤ì • ============
    stats_lock = Lock()
    journal_lock = Lock()
    task_queue: Queue = Queue(maxsize=QUEUE_MAXSIZE)

    def worker():
        """íŒŒì¼ ë³µì‚¬ Worker ìŠ¤ë ˆë“œ"""
        while True:
            if cancel_event.is_set() and task_queue.empty():
                break
            try:
                src_file, dst_file, rel_path = task_queue.get(timeout=0.5)
            except Empty:
                if cancel_event.is_set():
                    break
                continue

            try:
                try:
                    if dst_file.exists() and is_same_file(src_file, dst_file):
                        with stats_lock:
                            stats.skipped_same += 1
                        add_processed_file_safe(rel_path)
                        report_progress()
                        continue
                except Exception as e:
                    logger.error(f"[ERROR] same file check ì‹¤íŒ¨: {src_file} -> {dst_file}: {e}")
                    with stats_lock:
                        stats.copy_failed += 1
                    report_progress()
                    continue

                if cancel_event.is_set():
                    logger.info(f"[CANCELLED] ìŠ¤í‚µ (resume ì‹œ ì¬ì²˜ë¦¬ë¨): {src_file}")
                    # ì£¼ì˜: checkpointì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ â†’ resume ì‹œ ë‹¤ì‹œ ì²˜ë¦¬ë¨
                    report_progress()
                    continue

                ok = copy_with_retry(
                    src_file,
                    dst_file,
                    verify=job.verify,
                    journal=journal,
                    stats=stats,
                    stats_lock=stats_lock,
                    journal_lock=journal_lock,
                    dry_run=dry_run,
                )
                if ok:
                    add_processed_file_safe(rel_path)
            except Exception as e:
                logger.error(f"[WORKER ERROR] {src_file} -> {dst_file}: {e}")
                with stats_lock:
                    stats.copy_failed += 1
            finally:
                report_progress()
                task_queue.task_done()

    # Worker ìŠ¤ë ˆë“œ ì‹œì‘
    logger.info(f"ğŸ”§ ë©€í‹°ìŠ¤ë ˆë“œ Worker Pool ì´ˆê¸°í™”: {num_threads}ê°œ ìŠ¤ë ˆë“œ")
    workers: List[Thread] = []
    for i in range(num_threads):
        t = Thread(target=worker, daemon=True, name=f"Worker-{i+1}")
        t.start()
        workers.append(t)
    logger.info(f"âœ“ {num_threads}ê°œ Worker ìŠ¤ë ˆë“œ ì‹œì‘ ì™„ë£Œ")
    
    try:
        # ========== STAGE 1: íŒŒì¼ ìŠ¤ìº” ë° ê³„íš ==========
        update_stage("STAGE_1_SCAN", "running")
        logger.info("=" * 60)
        logger.info("STAGE 1: íŒŒì¼ ìŠ¤ìº” ë° ì‘ì—… ê³„íš ìˆ˜ë¦½")
        logger.info("=" * 60)
        
        scan_count = 0
        scan_progress_interval = 10000  # 10,000ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        last_scan_report = 0
        skipped_dirs = 0
        
        for root, dirs, files in os.walk(job.source):
            if tui is not None:
                tui.refresh_if_dirty()

            root_path = Path(root)
            
            # Resume ìµœì í™”: ì™„ë£Œëœ ë””ë ‰í† ë¦¬ëŠ” ì•„ì˜ˆ ìŠ¤ìº”í•˜ì§€ ì•ŠìŒ!
            if cp is not None:
                rel_root_str = root_path.relative_to(job.source).as_posix()
                if rel_root_str in cp.get("processed_dirs", set()):
                    skipped_dirs += 1
                    dirs[:] = []  # í•˜ìœ„ ë””ë ‰í† ë¦¬ë„ ìŠ¤ìº”í•˜ì§€ ì•ŠìŒ
                    logger.debug(f"[RESUME SKIP DIR] {rel_root_str}")
                    continue  # ì´ ë””ë ‰í† ë¦¬ ì „ì²´ ìŠ¤í‚µ!

            if tui is not None and tui.check_cancel_key():
                cancel_event.set()
                cancelled = True
                logger.info("[CANCEL] ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ íŒŒì¼ ìŠ¤ìº” ì¤‘ë‹¨")
                break

            dirs[:] = [d for d in dirs if not path_matches_patterns(root_path / d, job.exclude)]

            rel_root = root_path.relative_to(job.source)
            dest_root = job.destination / rel_root

            ensure_dir(dest_root, journal=journal, stats=stats, dry_run=dry_run)

            # ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ ì²˜ë¦¬
            dir_files_queued = 0
            for file in files:
                scan_count += 1
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥ (ëŒ€ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì‹œ)
                if scan_count - last_scan_report >= scan_progress_interval:
                    skip_info = f", {skipped_dirs:,}ê°œ ë””ë ‰í† ë¦¬ ìŠ¤í‚µ" if skipped_dirs > 0 else ""
                    logger.info(f"[SCAN] ì§„í–‰ ì¤‘... {scan_count:,}ê°œ íŒŒì¼ ìŠ¤ìº” ì™„ë£Œ{skip_info}")
                    last_scan_report = scan_count
                
                if tui is not None:
                    tui.refresh_if_dirty()
                    if tui.check_cancel_key():
                        cancel_event.set()
                        cancelled = True
                        logger.info("[CANCEL] ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ íŒŒì¼ ìŠ¤ìº” ì¤‘ë‹¨ (inner loop)")
                        break

                src_file = root_path / file
                if path_matches_patterns(src_file, job.exclude):
                    with stats_lock:
                        stats.skipped_excluded += 1
                    continue

                rel_path = src_file.relative_to(job.source).as_posix()

                # Resume: ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì€ ê±´ë„ˆëœ€
                if cp is not None and rel_path in cp["processed"]:
                    # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì€ íì— ë„£ì§€ ì•ŠìŒ
                    continue

                dst_file = dest_root / file

                task_queue.put((src_file, dst_file, rel_path))
                dir_files_queued += 1
            
            # ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìœ¼ë©´ ë””ë ‰í† ë¦¬ ì™„ë£Œ í‘œì‹œ
            if cp is not None and dir_files_queued == 0 and len(files) > 0:
                rel_dir = root_path.relative_to(job.source).as_posix()
                cp["processed_dirs"].add(rel_dir)

            if cancelled:
                break
                
        # STAGE 1 ì™„ë£Œ
        queued_count = task_queue.qsize()
        if skipped_dirs > 0:
            logger.info(f"[SCAN] ì™„ë£Œ: ì´ {scan_count:,}ê°œ íŒŒì¼ ìŠ¤ìº” (Resume: {skipped_dirs:,}ê°œ ë””ë ‰í† ë¦¬ ìŠ¤í‚µ), {queued_count:,}ê°œ ì‘ì—… ëŒ€ê¸°ì—´ì— ì¶”ê°€")
        else:
            logger.info(f"[SCAN] ì™„ë£Œ: ì´ {scan_count:,}ê°œ íŒŒì¼ ìŠ¤ìº”, {queued_count:,}ê°œ ì‘ì—… ëŒ€ê¸°ì—´ì— ì¶”ê°€")
        update_stage("STAGE_1_SCAN", "completed", 
                    items_total=total_files, items_processed=scan_count)
        
        # ========== STAGE 2: ë©€í‹°ìŠ¤ë ˆë“œ íŒŒì¼ ë³µì‚¬ ==========
        update_stage("STAGE_2_COPY", "running", items_total=total_files)
        logger.info("=" * 60)
        logger.info(f"STAGE 2: ë©€í‹°ìŠ¤ë ˆë“œ íŒŒì¼ ë³µì‚¬")
        logger.info(f"   Worker ìŠ¤ë ˆë“œ: {num_threads}ê°œ")
        logger.info(f"   ì²˜ë¦¬ ëŒ€ìƒ: {queued_count:,}ê°œ íŒŒì¼")
        logger.info("=" * 60)

        if not cancelled:
            task_queue.join()
            update_stage("STAGE_2_COPY", "completed", 
                        items_processed=current_processed)
        else:
            logger.info("[CANCEL] íì— ë‚¨ì€ ì‘ì—…ì€ ìŠ¤ë ˆë“œì—ì„œ ì •ë¦¬ í›„ ì¢…ë£Œ ì˜ˆì •")
            update_stage("STAGE_2_COPY", "failed", error="ì‚¬ìš©ì ì·¨ì†Œ")

        # ========== STAGE 3: ì •ë¦¬ ë° ë™ê¸°í™” ==========
        if cancelled:
            logger.info("ì‚¬ìš©ì ì·¨ì†Œë¡œ STAGE 3, 4ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            update_stage("STAGE_3_CLEANUP", "failed", error="ì‚¬ìš©ì ì·¨ì†Œ")
            update_stage("STAGE_4_SNAPSHOT", "failed", error="ì‚¬ìš©ì ì·¨ì†Œ")
            journal.status = "cancelled"
            save_journal(journal, journal_file, destination_root=job.destination)
            if cp is not None and not dry_run:
                cp["status"] = "incomplete"
                save_checkpoint(cp)
            logger.info(f"=== Job ì·¨ì†Œë¨: {job.name} (status=cancelled) ===")
            if tui is not None:
                tui.update_progress(current_processed * 100 // (total_files or 1), current_processed, total_files)
                tui.refresh_if_dirty()
            return

        update_stage("STAGE_3_CLEANUP", "running")
        logger.info("=" * 60)
        logger.info("STAGE 3: ì •ë¦¬ ë° ë™ê¸°í™” (ë¶ˆí•„ìš” íŒŒì¼ ì²˜ë¦¬)")
        logger.info("=" * 60)
        
        cleanup_count = 0
        if job.mode in ("clone", "safety_net"):
            for root, dirs, files in os.walk(job.destination):
                if tui is not None:
                    tui.refresh_if_dirty()

                root_path = Path(root)

                if any(x in root_path.parts for x in (".Rollback", ".SafetyNet")):
                    dirs[:] = []
                    continue

                dirs[:] = [d for d in dirs if not path_matches_patterns(root_path / d, job.exclude)]

                rel_root = root_path.relative_to(job.destination)
                src_root = job.source / rel_root

                for file in files:
                    dst_file = root_path / file
                    if path_matches_patterns(dst_file, job.exclude):
                        continue

                    rel_file = dst_file.relative_to(job.destination)
                    src_file = job.source / rel_file

                    if not src_file.exists():
                        if job.mode == "clone":
                            backup_path = Path(journal.rollback_root) / dst_file.relative_to(job.destination)
                            if dry_run:
                                logger.info(f"[DELETE (dry-run)] {dst_file}")
                            else:
                                try:
                                    backup_path.parent.mkdir(parents=True, exist_ok=True)
                                    logger.info(f"[BACKUP(before delete)] {dst_file} -> {backup_path}")
                                    shutil.move(str(dst_file), str(backup_path))
                                    journal.ops.append(JournalOp(
                                        action="delete_file",
                                        target=str(dst_file),
                                        backup=str(backup_path),
                                    ))
                                    stats.deleted_files += 1
                                except Exception as e:
                                    logger.error(f"[DELETE BACKUP ì‹¤íŒ¨] {dst_file}: {e}")
                        elif job.mode == "safety_net":
                            try:
                                sn_path = move_to_safety_net(dst_file, job.destination, dry_run=dry_run)
                                journal.ops.append(JournalOp(
                                    action="delete_file",
                                    target=str(dst_file),
                                    backup=str(sn_path),
                                ))
                                stats.safetynet_files += 1
                                cleanup_count += 1
                            except Exception as e:
                                logger.error(f"[SafetyNet ì´ë™ ì‹¤íŒ¨] {dst_file}: {e}")

            if job.mode == "clone" and not dry_run:
                for root, dirs, _files in os.walk(job.destination, topdown=False):
                    root_path = Path(root)
                    if any(x in root_path.parts for x in (".Rollback", ".SafetyNet")):
                        continue
                    for d in dirs:
                        dir_path = root_path / d
                        try:
                            dir_path.rmdir()
                            logger.info(f"[RMDIR] {dir_path}")
                            journal.ops.append(JournalOp(action="delete_file",
                                                         target=str(dir_path),
                                                         backup=None))
                        except OSError:
                            pass
        
        update_stage("STAGE_3_CLEANUP", "completed", 
                    items_processed=cleanup_count)

        # ========== STAGE 4: ìŠ¤ëƒ…ìƒ· ìƒì„± ==========
        update_stage("STAGE_4_SNAPSHOT", "running")
        logger.info("=" * 60)
        logger.info("STAGE 4: ìŠ¤ëƒ…ìƒ· ë° ìš”ì•½ ìƒì„±")
        logger.info("=" * 60)

        if total_files > 0 and current_processed < total_files:
            with progress_lock:
                current_processed = total_files
                if tui is not None:
                    tui.update_progress(100, current_processed, total_files)
                logger.info(
                    f"[PROGRESS] {job.name}: 100% ({current_processed}/{total_files})"
                )

        journal.status = "success"
        save_journal(journal, journal_file, destination_root=job.destination)

        if cp is not None and not dry_run:
            cp["status"] = "complete"
            save_checkpoint(cp)

        if not dry_run:
            snapshot_file = build_snapshot(job, journal, log_dir)
            summary_file = write_summary(job, journal, stats, log_dir)
            logger.info(f"ìŠ¤ëƒ…ìƒ· íŒŒì¼: {snapshot_file}")
            logger.info(f"ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
        
        update_stage("STAGE_4_SNAPSHOT", "completed")
        
        # ========== ì „ì²´ Stage ìš”ì•½ ==========
        logger.info("=" * 60)
        logger.info("ì „ì²´ Stage ì‹¤í–‰ ê²°ê³¼")
        logger.info("=" * 60)
        for stage_key, stage in stages.items():
            status_icon = "âœ“" if stage.status == "completed" else "âœ—" if stage.status == "failed" else "â—‹"
            logger.info(f"{status_icon} {stage.stage_name}: {stage.status} "
                       f"({stage.start_time or '-'} ~ {stage.end_time or '-'})")
        logger.info("=" * 60)

        logger.info(f"=== Job ì„±ê³µ: {job.name} ===")

        if tui is not None:
            tui.refresh_if_dirty()

    except Exception as e:
        logger.error(f"Job ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        logger.error(f"ìƒì„¸ ì—ëŸ¬: {traceback.format_exc()}")
        try:
            rollback_journal(journal, dry_run=dry_run)
            journal.status = "rolled_back"
        except Exception as re:
            logger.error(f"ìë™ ë¡¤ë°± ì‹¤íŒ¨: {re}")
            journal.status = "rollback_failed"
        finally:
            save_journal(journal, journal_file, destination_root=job.destination)
        logger.error(f"=== Job ì‹¤íŒ¨ ë° ë¡¤ë°± ì²˜ë¦¬ ì™„ë£Œ (status={journal.status}) ===")


# ================ CLI (ê¸°ì¡´) =================

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="CCC + SuperDuper ìŠ¤íƒ€ì¼ì˜ í”„ë¡œê¸‰ ë””ìŠ¤í¬ ë°±ì—…/ë™ê¸°í™” ìŠ¤í¬ë¦½íŠ¸"
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    backup_parser = subparsers.add_parser("backup", help="ë°±ì—… ì‹¤í–‰ (TUI + ë©€í‹°ìŠ¤ë ˆë“œ)")
    backup_parser.add_argument("-c", "--config", required=True, help="ë°±ì—… ì„¤ì • JSON íŒŒì¼ ê²½ë¡œ")
    backup_parser.add_argument("-j", "--job", help="ì‹¤í–‰í•  Job ì´ë¦„ (ìƒëµ ì‹œ ì „ì²´ Job ì‹¤í–‰)")
    backup_parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ë³µì‚¬/ì‚­ì œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰")
    backup_parser.add_argument("--log-dir", help="ë¡œê·¸/ì €ë„ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./logs)")
    backup_parser.add_argument("--resume", action="store_true",
                               help="ì´ì „ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•´ ì¤‘ë‹¨ëœ ë°±ì—…ì„ ì´ì–´ì„œ ì‹¤í–‰")

    rollback_parser = subparsers.add_parser("rollback", help="ê¸°ì¡´ ì €ë„ íŒŒì¼ì„ ì´ìš©í•´ ë¡¤ë°± ì‹¤í–‰")
    rollback_parser.add_argument("-f", "--journal-file", required=True, help="ì €ë„ JSON íŒŒì¼ ê²½ë¡œ")
    rollback_parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ë¡¤ë°± ì—†ì´ ì‹œë®¬ë ˆì´ì…˜")

    return parser.parse_args()


def _run_backup(args: argparse.Namespace, tui: Optional[SimpleTUI] = None) -> Path:
    config_path = Path(args.config).expanduser()
    if not config_path.exists():
        print(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}", file=sys.stderr)
        sys.exit(1)

    if args.log_dir:
        log_dir = Path(args.log_dir).expanduser()
    else:
        log_dir = Path(__file__).resolve().parent / "logs"
    log_dir.mkdir(parents=True, exist_ok=True)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"disk_sync_pro_{ts}.log"

    use_tui = tui is not None
    setup_logger(log_file=log_file, verbose=not use_tui, use_tui=use_tui, tui_obj=tui)

    logger.info(f"ì„¤ì • íŒŒì¼: {config_path}")
    logger.info(f"ë¡œê·¸ íŒŒì¼: {log_file}")

    jobs = load_config(config_path)
    if args.job:
        jobs = [job for job in jobs if job.name == args.job]
        if not jobs:
            logger.error(f"í•´ë‹¹ ì´ë¦„ì˜ Jobì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.job}")
            sys.exit(1)

    for job in jobs:
        perform_backup(job, dry_run=args.dry_run, log_dir=log_dir, resume=args.resume, tui=tui)

    logger.info("ëª¨ë“  Jobì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    return log_dir


def main_backup(args: argparse.Namespace) -> None:
    if sys.stdout.isatty() and curses is not None:
        def curses_main(stdscr):
            tui = SimpleTUI(stdscr)
            _run_backup(args, tui=tui)
            tui.add_log_line("ë°±ì—…ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            tui.refresh_if_dirty()
            stdscr.nodelay(False)
            stdscr.getch()
        curses.wrapper(curses_main)
    else:
        _run_backup(args, tui=None)


def main_rollback(args: argparse.Namespace) -> None:
    journal_path = Path(args.journal_file).expanduser()
    if not journal_path.exists():
        print(f"ì €ë„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {journal_path}", file=sys.stderr)
        sys.exit(1)

    log_dir = journal_path.parent
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"disk_sync_pro_rollback_{ts}.log"
    setup_logger(log_file=log_file, verbose=True)

    logger.info(f"ì €ë„ íŒŒì¼: {journal_path}")
    journal = load_journal(journal_path)
    logger.info(f"Journal status: {journal.status}")

    dest_root = Path(journal.dest_root) if journal.dest_root else None

    try:
        rollback_journal(journal, dry_run=args.dry_run)
        if not args.dry_run:
            journal.status = "rolled_back"
            save_journal(journal, journal_path, destination_root=dest_root)
    except Exception as e:
        logger.error(f"ë¡¤ë°± ì¤‘ ì—ëŸ¬: {e}")
        if not args.dry_run:
            journal.status = "rollback_failed"
            save_journal(journal, journal_path, destination_root=dest_root)


# ================ logs ê¸°ë°˜ ë„ìš°ë¯¸ (ë©”ë‰´ì—ì„œ ì‚¬ìš©) =================

def get_latest_journal(log_dir: Path) -> Optional[Path]:
    journals = sorted(log_dir.glob("journal_*.json"))
    if not journals:
        return None
    return journals[-1]


def safe_addstr(stdscr, row: int, col: int, text: str, attr=0):
    """
    ì•ˆì „í•œ addstr ë˜í¼ - ì „ì—­ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    - ìë™ í™”ë©´ í¬ê¸° ê°ì§€
    - í•œê¸€/ìœ ë‹ˆì½”ë“œ ì•ˆì „ ì²˜ë¦¬
    - ì—ëŸ¬ ë³µêµ¬
    """
    try:
        rows, cols = stdscr.getmaxyx()
        if row >= rows or col >= cols:
            return
        
        available_width = cols - col - 1
        if available_width <= 0:
            return
        
        # ë¬¸ìì—´ ê¸¸ì´ ì œí•œ
        safe_text = text[:available_width]
        
        # ì ì§„ì ìœ¼ë¡œ ì¤„ì´ë©° ì•ˆì „í•˜ê²Œ ì¶œë ¥
        while len(safe_text) > 0:
            try:
                stdscr.addstr(row, col, safe_text, attr)
                break
            except Exception:
                safe_text = safe_text[:-1]
    except Exception:
        pass


def show_text_screen(stdscr, title: str, lines: List[str]):
    """í…ìŠ¤íŠ¸ í™”ë©´ í‘œì‹œ (ëª©ë¡ ë“±)"""
    stdscr.clear()
    rows, cols = stdscr.getmaxyx()
    header = f" {title} ".center(cols, "=")
    safe_addstr(stdscr, 0, 0, header)

    max_lines = rows - 3
    for i, line in enumerate(lines[:max_lines], start=2):
        safe_addstr(stdscr, i, 0, line)

    footer = "ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ì´ì „ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤."
    safe_addstr(stdscr, rows - 1, 0, footer)
    stdscr.refresh()
    stdscr.getch()


def show_journal_list_screen(stdscr, log_dir: Path):
    lines: List[str] = []
    journals = sorted(log_dir.glob("journal_*.json"))
    if not journals:
        lines.append("journal_*.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for j in journals[-100:]:
            try:
                data = load_journal(j)
                lines.append(
                    f"{j.name}  | job={data.job_name} | ts={data.timestamp} | status={data.status}"
                )
            except Exception as e:
                lines.append(f"{j.name}  | (ì½ê¸° ì‹¤íŒ¨: {e})")

    show_text_screen(stdscr, "Journal ëª©ë¡", lines)


def show_snapshot_list_screen(stdscr, log_dir: Path):
    lines: List[str] = []
    snapshots_root = log_dir / "snapshots"
    if not snapshots_root.exists():
        lines.append("snapshots ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        show_text_screen(stdscr, "Snapshot ëª©ë¡", lines)
        return

    for job_dir in sorted(snapshots_root.iterdir()):
        if not job_dir.is_dir():
            continue
        index_file = job_dir / "index.json"
        if not index_file.exists():
            lines.append(f"[{job_dir.name}] index.json ì—†ìŒ")
            continue
        try:
            with index_file.open("r", encoding="utf-8") as f:
                index = json.load(f)
            lines.append(f"=== Job: {job_dir.name} (snapshots: {len(index)}) ===")
            for entry in index[-20:]:
                lines.append(
                    f"  ts={entry.get('timestamp')} | file={entry.get('snapshot_file')} "
                    f"| count={entry.get('file_count')} | at={entry.get('generated_at')}"
                )
        except Exception as e:
            lines.append(f"[{job_dir.name}] index.json ì½ê¸° ì‹¤íŒ¨: {e}")

    if not lines:
        lines.append("í‘œì‹œí•  ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŠµë‹ˆë‹¤.")

    show_text_screen(stdscr, "Snapshot ëª©ë¡", lines)


# ================ config ì„ íƒ / ë©”ë‰´ ê¸°ë°˜ backup ì‹¤í–‰ =================

def curses_input_string(stdscr, row: int, col: int, prompt: str = "", maxlen: int = 80, 
                        show_cursor: bool = True) -> str:
    """
    ì•ˆì „í•œ ë¬¸ìì—´ ì…ë ¥ í•¨ìˆ˜
    - í•œê¸€/ìœ ë‹ˆì½”ë“œ ì™„ì „ ì§€ì›
    - í™”ë©´ í¬ê¸° ìë™ ê°ì§€
    - ì—ëŸ¬ ë³µêµ¬
    """
    try:
        rows, cols = stdscr.getmaxyx()
        
        # í–‰/ì—´ ë²”ìœ„ ì²´í¬
        if row >= rows or col >= cols:
            return ""
        
        # ë¼ì¸ í´ë¦¬ì–´ ë° í”„ë¡¬í”„íŠ¸ ì¶œë ¥
        try:
            stdscr.move(row, 0)
            stdscr.clrtoeol()
        except Exception:
            pass
        
        if prompt:
            safe_addstr(stdscr, row, col, prompt)
            input_col = col + len(prompt) + 1
        else:
            input_col = col
        
        # ì…ë ¥ ê°€ëŠ¥í•œ ìµœëŒ€ ë„ˆë¹„ ê³„ì‚°
        available_width = max(1, cols - input_col - 2)
        input_width = min(maxlen, available_width)
        
        if input_width <= 0:
            return ""
        
        # ì»¤ì„œ í‘œì‹œ
        if show_cursor:
            try:
                curses.curs_set(1)
            except Exception:
                pass
        
        # ì…ë ¥ ìœ„ì¹˜ë¡œ ì´ë™
        try:
            stdscr.move(row, input_col)
        except Exception:
            pass
        
        stdscr.refresh()
        
        # ì…ë ¥ ë°›ê¸°
        curses.echo()
        try:
            input_bytes = stdscr.getstr(row, input_col, input_width)
            result = input_bytes.decode("utf-8", errors="ignore").strip()
        except Exception as e:
            result = ""
        finally:
            curses.noecho()
            if show_cursor:
                try:
                    curses.curs_set(0)
                except Exception:
                    pass
        
        return result
        
    except Exception:
        return ""


def curses_input_line(stdscr, prompt: str, default: str = "") -> str:
    """
    ì „ì²´ í™”ë©´ì„ ì‚¬ìš©í•œ í•œ ì¤„ ì…ë ¥
    """
    try:
        rows, cols = stdscr.getmaxyx()
        stdscr.clear()
        
        # ì œëª©
        title = " ì…ë ¥ ".center(cols, "=")
        safe_addstr(stdscr, 0, 0, title)
        
        # í”„ë¡¬í”„íŠ¸
        safe_addstr(stdscr, 2, 2, prompt)
        
        # ê¸°ë³¸ê°’ í‘œì‹œ
        if default:
            safe_addstr(stdscr, 3, 2, f"(í˜„ì¬: {default})")
            input_row = 5
        else:
            input_row = 4
        
        safe_addstr(stdscr, input_row, 2, "ì…ë ¥: ")
        
        # ì•ˆë‚´
        safe_addstr(stdscr, rows - 2, 2, "Enterë¥¼ ëˆŒëŸ¬ ì…ë ¥ ì™„ë£Œ | ë¹ˆ ê°’ì´ë©´ ì·¨ì†Œ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©")
        
        stdscr.refresh()
        
        # ì…ë ¥ ë°›ê¸°
        result = curses_input_string(stdscr, input_row, 8, "", maxlen=cols - 12)
        
        return result if result else default
        
    except Exception:
        return default


def curses_prompt(stdscr, prompt: str, maxlen: int = 40) -> str:
    """
    í•˜ë‹¨ ìƒíƒœë°”ì—ì„œ ì§§ì€ ì…ë ¥ ë°›ê¸°
    """
    try:
        rows, cols = stdscr.getmaxyx()
        prompt_row = rows - 1
        
        # í”„ë¡¬í”„íŠ¸ ì¶œë ¥
        try:
            stdscr.move(prompt_row, 0)
            stdscr.clrtoeol()
        except Exception:
            pass
        
        safe_addstr(stdscr, prompt_row, 0, prompt)
        
        # ì…ë ¥
        result = curses_input_string(stdscr, prompt_row, len(prompt), "", maxlen=maxlen)
        
        return result
        
    except Exception:
        return ""


def find_config_files() -> List[Path]:
    """
    í˜„ì¬ ë””ë ‰í† ë¦¬ì™€ ./configs ì—ì„œ *.json ê²€ìƒ‰
    """
    configs: List[Path] = []
    base_dir = Path(__file__).resolve().parent
    candidates = [Path.cwd(), base_dir / "configs"]
    seen = set()
    for d in candidates:
        if not d.exists():
            continue
        for p in sorted(d.glob("*.json")):
            if p.resolve() not in seen:
                seen.add(p.resolve())
                configs.append(p)
    return configs


def get_config_preview_lines(config_path: Path) -> List[str]:
    """
    config JSON ì•ˆì— ì–´ë–¤ Job ì´ ë“¤ì–´ìˆëŠ”ì§€ ê°„ë‹¨íˆ ìš”ì•½í•´ì„œ ë³´ì—¬ì£¼ëŠ” ìš©ë„.
    """
    lines: List[str] = []
    try:
        jobs = load_config(config_path)
    except Exception as e:
        return [f"  (config ì½ê¸° ì‹¤íŒ¨: {e})"]

    if not jobs:
        return ["  (jobs: 0)"]

    lines.append(f"  jobs: {len(jobs)}")
    max_show = 3
    for job in jobs[:max_show]:
        lines.append(f"    - {job.name} [{job.mode}]")
        lines.append(f"      src={job.source}")
        lines.append(f"      dst={job.destination}")
    if len(jobs) > max_show:
        lines.append(f"    ... +{len(jobs) - max_show} more job(s)")
    return lines


def curses_get_line(stdscr, prompt: str, default: str = "") -> str:
    """í•œ ì¤„ ì…ë ¥ ë˜í¼"""
    return curses_input_line(stdscr, prompt, default)




def interactive_select_config_curses(stdscr) -> Optional[Path]:
    """config íŒŒì¼ ì„ íƒ í™”ë©´"""
    configs = find_config_files()
    rows, cols = stdscr.getmaxyx()

    while True:
        stdscr.clear()
        title = " Config ì„ íƒ ".center(cols, "=")
        safe_addstr(stdscr, 0, 0, title)

        if not configs:
            safe_addstr(stdscr, 2, 0, "ìë™ìœ¼ë¡œ ì°¾ì€ config JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            safe_addstr(stdscr, 4, 0, "ì§ì ‘ ê²½ë¡œë¥¼ ì…ë ¥í•˜ë ¤ë©´ 'p' í‚¤ë¥¼, ì·¨ì†Œí•˜ë ¤ë©´ 'q' í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”.")
            stdscr.refresh()
            ch = stdscr.getch()
            if ch in (ord('q'), ord('Q')):
                return None
            elif ch in (ord('p'), ord('P')):
                path_str = curses_input_line(stdscr, "config JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
                if not path_str:
                    return None
                p = Path(path_str).expanduser()
                if not p.exists():
                    show_text_screen(stdscr, "ì˜¤ë¥˜", [f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {p}"])
                    return None
                return p
            else:
                continue

        safe_addstr(stdscr, 2, 0, "ì•„ë˜ì—ì„œ config JSON íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
        safe_addstr(stdscr, 3, 0, "(ê° config ì•„ë˜ì— jobs ìš”ì•½ì´ í•¨ê»˜ í‘œì‹œë©ë‹ˆë‹¤.)")

        start_row = 5
        row = start_row

        for idx, cfg in enumerate(configs, start=1):
            if row >= rows - 4:
                break

            line = f"{idx}) {cfg}"
            safe_addstr(stdscr, row, 0, line)
            row += 1

            preview_lines = get_config_preview_lines(cfg)
            for pl in preview_lines:
                if row >= rows - 4:
                    break
                safe_addstr(stdscr, row, 0, pl)
                row += 1

            if row < rows - 4:
                row += 1

        safe_addstr(stdscr, rows - 3, 0, "P) ì§ì ‘ ê²½ë¡œ ì…ë ¥")
        safe_addstr(stdscr, rows - 2, 0, "Q) ì·¨ì†Œ")

        choice = curses_prompt(stdscr, "ì„ íƒ ë²ˆí˜¸ ë˜ëŠ” P/Q ì…ë ¥ í›„ Enter: ")

        if not choice:
            continue
        if choice.lower() == 'q':
            return None
        if choice.lower() == 'p':
            path_str = curses_input_line(stdscr, "config JSON íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
            if not path_str:
                return None
            p = Path(path_str).expanduser()
            if not p.exists():
                show_text_screen(stdscr, "ì˜¤ë¥˜", [f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {p}"])
                return None
            return p

        try:
            idx = int(choice)
        except ValueError:
            continue

        if 1 <= idx <= len(configs):
            return configs[idx - 1]


def interactive_select_job_curses(stdscr, config_path: Path) -> (Optional[str], bool, bool, bool):
    """Job ì„ íƒ í™”ë©´"""
    jobs = load_config(config_path)
    rows, cols = stdscr.getmaxyx()

    if not jobs:
        show_text_screen(stdscr, "ì˜¤ë¥˜", ["config ì— jobs ê°€ ì—†ìŠµë‹ˆë‹¤."])
        return None, False, False, True

    while True:
        stdscr.clear()
        title = f" Job ì„ íƒ ({config_path.name}) ".center(cols, "=")
        safe_addstr(stdscr, 0, 0, title)

        safe_addstr(stdscr, 2, 0, "ì‹¤í–‰í•  Job ì„ ì„ íƒí•˜ì„¸ìš”. 0ë²ˆì€ ì „ì²´ Job ì‹¤í–‰ì…ë‹ˆë‹¤.")
        start_row = 4
        safe_addstr(stdscr, start_row - 1, 0, "0) ëª¨ë“  Job ì‹¤í–‰")

        for idx, job in enumerate(jobs, start=1):
            line = f"{idx}) {job.name}  (src={job.source}, dst={job.destination}, mode={job.mode})"
            if start_row + idx >= rows - 4:
                break
            safe_addstr(stdscr, start_row + idx - 1, 0, line)

        safe_addstr(stdscr, rows - 3, 0, "Q) ì·¨ì†Œ")

        sel = curses_prompt(stdscr, "ì„ íƒ ë²ˆí˜¸ ì…ë ¥ í›„ Enter: ")

        if not sel:
            continue
        if sel.lower() == 'q':
            return None, False, False, True

        try:
            idx = int(sel)
        except ValueError:
            continue

        if idx == 0:
            job_name = None
        elif 1 <= idx <= len(jobs):
            job_name = jobs[idx - 1].name
        else:
            continue

        stdscr.clear()
        safe_addstr(stdscr, 0, 0, "Dry-run ëª¨ë“œë¡œ ì‹¤í–‰í• ê¹Œìš”? (ë³€ê²½ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜ë§Œ ìˆ˜í–‰) [y/N]")
        stdscr.refresh()
        ch = stdscr.getch()
        dry_run = (ch in (ord('y'), ord('Y')))

        stdscr.clear()
        safe_addstr(stdscr, 0, 0, "ì´ì „ ì²´í¬í¬ì¸íŠ¸(resume)ë¥¼ ì‚¬ìš©í• ê¹Œìš”? [y/N]")
        stdscr.refresh()
        ch = stdscr.getch()
        resume = (ch in (ord('y'), ord('Y')))

        return job_name, dry_run, resume, False


def interactive_config_editor_curses(stdscr):
    """Config í¸ì§‘ í™”ë©´"""
    rows, cols = stdscr.getmaxyx()
    stdscr.clear()
    title = " Config ìƒì„±/ìˆ˜ì • ".center(cols, "=")
    safe_addstr(stdscr, 0, 0, title)
    safe_addstr(stdscr, 2, 0, "ê¸°ì¡´ config ìˆ˜ì •(E), ìƒˆ config ìƒì„±(N), ì·¨ì†Œ(Q)")
    stdscr.refresh()

    choice = curses_prompt(stdscr, "ì„ íƒ(E/N/Q): ").lower()
    if not choice or choice == 'q':
        return

    if choice == 'n':
        path_str = curses_input_line(stdscr, "ìƒˆë¡œ ë§Œë“¤ config JSON ê²½ë¡œ (ì˜ˆ: ./configs/my_backup.json):")
        if not path_str:
            return
        config_path = Path(path_str).expanduser()
        jobs: List[BackupJob] = []
    else:
        config_path = interactive_select_config_curses(stdscr)
        if config_path is None:
            return
        try:
            jobs = load_config(config_path)
        except Exception as e:
            show_text_screen(stdscr, "ì˜¤ë¥˜", [f"config ì½ê¸° ì‹¤íŒ¨: {e}"])
            return

    while True:
        stdscr.clear()
        title = f" Config í¸ì§‘: {config_path.name} ".center(cols, "=")
        safe_addstr(stdscr, 0, 0, title)

        if not jobs:
            safe_addstr(stdscr, 2, 0, "í˜„ì¬ ë“±ë¡ëœ Job ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            safe_addstr(stdscr, 2, 0, "ìˆ˜ì •í•  Job ì„ ì„ íƒí•˜ì„¸ìš”.")
            safe_addstr(stdscr, 3, 0, "0) ìƒˆ Job ì¶”ê°€")
            row = 4
            for idx, job in enumerate(jobs, start=1):
                line = f"{idx}) {job.name} (mode={job.mode}, src={job.source}, dst={job.destination})"
                if row >= rows - 4:
                    break
                safe_addstr(stdscr, row, 0, line)
                row += 1

        safe_addstr(stdscr, rows - 3, 0, "Q) ì·¨ì†Œ")
        sel = curses_prompt(stdscr, "ì„ íƒ ë²ˆí˜¸ ì…ë ¥ (0=ìƒˆ Job, Q=ì·¨ì†Œ): ").lower()

        if not sel or sel == 'q':
            return

        if sel == '0':
            job = BackupJob(
                name="",
                source=Path("."),
                destination=Path("."),
                mode="safety_net",
                exclude=[],
                safety_net_days=30,
                verify=False,
            )
            jobs.append(job)
            editing_job = job
        else:
            try:
                idx = int(sel)
            except ValueError:
                continue
            if not (1 <= idx <= len(jobs)):
                continue
            editing_job = jobs[idx - 1]

        def edit_field(label: str, current: str) -> str:
            value = curses_input_line(stdscr, label, default=current)
            return value if value else current

        editing_job.name = edit_field("Job ì´ë¦„", editing_job.name or "")
        editing_job.source = Path(edit_field("Source ê²½ë¡œ", str(editing_job.source))).expanduser()
        editing_job.destination = Path(edit_field("Destination ê²½ë¡œ", str(editing_job.destination))).expanduser()

        mode_val = edit_field("Mode (clone/sync/safety_net)", editing_job.mode)
        if mode_val in ("clone", "sync", "safety_net"):
            editing_job.mode = mode_val

        excl_str_current = ", ".join(editing_job.exclude) if editing_job.exclude else ""
        excl_str = edit_field("Exclude íŒ¨í„´ (ì‰¼í‘œ êµ¬ë¶„, ì˜ˆ: .DS_Store,*.tmp)", excl_str_current)
        if excl_str:
            editing_job.exclude = [x.strip() for x in excl_str.split(",") if x.strip()]

        days_str = edit_field("SafetyNet ë³´ê´€ì¼ ìˆ˜", str(editing_job.safety_net_days))
        if days_str:
            try:
                editing_job.safety_net_days = int(days_str)
            except ValueError:
                pass

        v_str = edit_field("í•´ì‹œ ê²€ì¦(verify) ì‚¬ìš©? (y/n)", "y" if editing_job.verify else "n")
        if v_str.lower() in ("y", "yes"):
            editing_job.verify = True
        elif v_str.lower() in ("n", "no"):
            editing_job.verify = False

        raw = {
            "jobs": [
                {
                    "name": j.name,
                    "source": str(j.source),
                    "destination": str(j.destination),
                    "mode": j.mode,
                    "exclude": j.exclude,
                    "safety_net_days": j.safety_net_days,
                    "verify": j.verify,
                }
                for j in jobs
            ]
        }
        try:
            config_path.parent.mkdir(parents=True, exist_ok=True)
            with config_path.open("w", encoding="utf-8") as f:
                json.dump(raw, f, indent=2, ensure_ascii=False)
        except Exception as e:
            show_text_screen(stdscr, "ì˜¤ë¥˜", [f"config ì €ì¥ ì‹¤íŒ¨: {e}"])
            return

        show_text_screen(
            stdscr,
            "Config ì €ì¥ ì™„ë£Œ",
            [
                f"íŒŒì¼: {config_path}",
                f"Job ìˆ˜: {len(jobs)}",
                "",
                "Config ì €ì¥ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.",
            ],
        )
        return


def interactive_backup_flow_curses(stdscr, base_log_dir: Path):
    config_path = interactive_select_config_curses(stdscr)
    if config_path is None:
        return

    job_name, dry_run, resume, cancelled = interactive_select_job_curses(stdscr, config_path)
    if cancelled:
        return

    args = SimpleNamespace(
        command="backup",
        config=str(config_path),
        job=job_name,
        dry_run=dry_run,
        log_dir=str(base_log_dir),
        resume=resume,
    )

    tui = SimpleTUI(stdscr)
    _run_backup(args, tui=tui)
    tui.add_log_line("ë°±ì—…ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
    tui.refresh_if_dirty()
    stdscr.nodelay(False)
    stdscr.getch()
    stdscr.nodelay(False)
    stdscr.clear()
    stdscr.refresh()


def interactive_latest_rollback_curses(stdscr, base_log_dir: Path):
    """ìµœê·¼ ì €ë„ ë¡¤ë°± í™”ë©´"""
    stdscr.clear()
    rows, cols = stdscr.getmaxyx()
    latest = get_latest_journal(base_log_dir)
    if latest is None:
        safe_addstr(stdscr, 0, 0, "ìµœê·¼ ì €ë„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        safe_addstr(stdscr, 2, 0, "ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
        stdscr.refresh()
        stdscr.getch()
        return

    try:
        j = load_journal(latest)
        safe_addstr(stdscr, 0, 0, f"ìµœê·¼ ì €ë„: {latest.name}")
        safe_addstr(stdscr, 1, 0, f"job={j.job_name}, ts={j.timestamp}, status={j.status}")
        safe_addstr(stdscr, 3, 0, "í•´ë‹¹ ì €ë„ë¡œ ë¡¤ë°±ì„ ì§„í–‰í• ê¹Œìš”? (y/N)")
        stdscr.refresh()
        c2 = stdscr.getch()
        if c2 in (ord('y'), ord('Y')):
            rollback_journal(journal=j, dry_run=False)
            j.status = "rolled_back"
            dest_root = Path(j.dest_root) if j.dest_root else None
            save_journal(j, latest, destination_root=dest_root)
            safe_addstr(stdscr, 5, 0, "ë¡¤ë°±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
            stdscr.refresh()
            stdscr.getch()
        else:
            safe_addstr(stdscr, 5, 0, "ë¡¤ë°±ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
            stdscr.refresh()
            stdscr.getch()
    except Exception as e:
        safe_addstr(stdscr, 0, 0, f"ì €ë„ ë¡œë“œ/ë¡¤ë°± ì¤‘ ì˜¤ë¥˜: {e}")
        safe_addstr(stdscr, 2, 0, "ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
        stdscr.refresh()
        stdscr.getch()


# ================ TUI ë©”ì¸ ë©”ë‰´ =================

def interactive_main_curses(stdscr):
    """ë©”ì¸ ë©”ë‰´ í™”ë©´"""
    try:
        curses.curs_set(0)
    except Exception:
        pass

    base_log_dir = Path(__file__).resolve().parent / "logs"
    base_log_dir.mkdir(parents=True, exist_ok=True)

    # ìƒ‰ìƒ ì´ˆê¸°í™”
    use_colors = False
    try:
        if curses.has_colors():
            curses.start_color()
            curses.use_default_colors()
            curses.init_pair(1, curses.COLOR_CYAN, -1)
            curses.init_pair(2, curses.COLOR_YELLOW, -1)
            use_colors = True
    except Exception:
        pass

    while True:
        stdscr.clear()
        rows, cols = stdscr.getmaxyx()
        
        # ì œëª©
        title = " DiskSyncPro - Main Menu "
        title_line = title.center(cols, "=")
        title_attr = curses.color_pair(1) | curses.A_BOLD if use_colors else curses.A_BOLD
        safe_addstr(stdscr, 0, 0, title_line, title_attr)
        
        # ë²„ì „ ì •ë³´
        version_line = "Professional Backup & Sync Tool v2.0"
        safe_addstr(stdscr, 1, (cols - len(version_line)) // 2, version_line)

        # ë©”ë‰´ ì˜µì…˜
        menu_lines = [
            ("", 0),
            ("ë©”ë‰´ë¥¼ ì„ íƒí•˜ì„¸ìš”:", 0),
            ("", 0),
            ("1) config ì„ íƒ í›„ ë°±ì—… ì‹¤í–‰", 2),
            ("2) ê°€ì¥ ìµœê·¼ Job ì €ë„ë¡œ ë¡¤ë°± ì‹¤í–‰", 2),
            ("3) journal_*.json ëª©ë¡ ë³´ê¸°", 2),
            ("4) snapshots/ ëª©ë¡ ë³´ê¸°", 2),
            ("5) config JSON ìƒì„±/ìˆ˜ì •", 2),
            ("", 0),
            ("Q) ì¢…ë£Œ", 2),
        ]

        row = 3
        for line, attr_type in menu_lines:
            if row >= rows - 2:
                break
            if attr_type == 2 and use_colors:
                attr = curses.color_pair(2)
            else:
                attr = 0
            safe_addstr(stdscr, row, 2, line, attr)
            row += 1

        # í•˜ë‹¨ ì•ˆë‚´
        help_line = "[1-5] ë©”ë‰´ ì„ íƒ  [Q] ì¢…ë£Œ  [ESC] ë’¤ë¡œê°€ê¸°"
        safe_addstr(stdscr, rows - 1, (cols - len(help_line)) // 2, help_line)

        stdscr.refresh()
        
        # í‚¤ ì…ë ¥ ëŒ€ê¸°
        try:
            ch = stdscr.getch()
        except Exception:
            continue

        if ch in (ord('q'), ord('Q'), 27):  # Q ë˜ëŠ” ESC
            break
        elif ch == ord('1'):
            try:
                interactive_backup_flow_curses(stdscr, base_log_dir)
            except Exception as e:
                show_text_screen(stdscr, "ì˜¤ë¥˜", [f"ë°±ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}"])
        elif ch == ord('2'):
            try:
                interactive_latest_rollback_curses(stdscr, base_log_dir)
            except Exception as e:
                show_text_screen(stdscr, "ì˜¤ë¥˜", [f"ë¡¤ë°± ì¤‘ ì˜¤ë¥˜: {e}"])
        elif ch == ord('3'):
            try:
                show_journal_list_screen(stdscr, base_log_dir)
            except Exception as e:
                show_text_screen(stdscr, "ì˜¤ë¥˜", [f"ì €ë„ ëª©ë¡ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}"])
        elif ch == ord('4'):
            try:
                show_snapshot_list_screen(stdscr, base_log_dir)
            except Exception as e:
                show_text_screen(stdscr, "ì˜¤ë¥˜", [f"ìŠ¤ëƒ…ìƒ· ëª©ë¡ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {e}"])
        elif ch == ord('5'):
            try:
                interactive_config_editor_curses(stdscr)
            except Exception as e:
                show_text_screen(stdscr, "ì˜¤ë¥˜", [f"ì„¤ì • í¸ì§‘ ì¤‘ ì˜¤ë¥˜: {e}"])
        
        # ì ì‹œ ëŒ€ê¸° (í‚¤ ì¤‘ë³µ ì…ë ¥ ë°©ì§€)
        try:
            stdscr.nodelay(True)
            while stdscr.getch() != -1:
                pass
            stdscr.nodelay(False)
        except Exception:
            pass


# ================ í…ìŠ¤íŠ¸ ëª¨ë“œ ë©”ì¸ ë©”ë‰´ (curses ë¶ˆê°€ ì‹œ) =================

def interactive_main_plain():
    base_log_dir = Path(__file__).resolve().parent / "logs"
    base_log_dir.mkdir(parents=True, exist_ok=True)

    while True:
        print("=" * 60)
        print(" DiskSyncPro - Main Menu (no curses) ".center(60))
        print("=" * 60)
        print("1) config ì„ íƒ í›„ ë°±ì—… ì‹¤í–‰")
        print("2) ê°€ì¥ ìµœê·¼ Job ì €ë„ë¡œ ë¡¤ë°± ì‹¤í–‰")
        print("3) journal_*.json ëª©ë¡ ë³´ê¸°")
        print("4) snapshots/ ëª©ë¡ ë³´ê¸°")
        print("5) config JSON ìƒì„±/ìˆ˜ì •")
        print("Q) ì¢…ë£Œ")
        choice = input("> ").strip().lower()

        if choice == 'q':
            break
        elif choice == '1':
            configs = find_config_files()
            if not configs:
                print("config JSON íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                path_str = input("ì§ì ‘ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì·¨ì†Œ: ë¹ˆ ì¤„): ").strip()
                if not path_str:
                    continue
                config_path = Path(path_str).expanduser()
            else:
                print("config íŒŒì¼ ëª©ë¡:")
                for idx, c in enumerate(configs, start=1):
                    print(f"{idx}) {c}")
                    for pl in get_config_preview_lines(c):
                        print(pl)
                    print()
                sel = input("ë²ˆí˜¸ ì„ íƒ (ë˜ëŠ” ì§ì ‘ ê²½ë¡œ ì…ë ¥): ").strip()
                try:
                    idx = int(sel)
                    config_path = configs[idx - 1]
                except (ValueError, IndexError):
                    config_path = Path(sel).expanduser()

            if not config_path.exists():
                print(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {config_path}")
                input("ê³„ì†í•˜ë ¤ë©´ Enter...")
                continue

            jobs = load_config(config_path)
            if not jobs:
                print("config ì— jobs ê°€ ì—†ìŠµë‹ˆë‹¤.")
                input("ê³„ì†í•˜ë ¤ë©´ Enter...")
                continue

            print("Job ëª©ë¡:")
            print("0) ëª¨ë“  Job ì‹¤í–‰")
            for idx, job in enumerate(jobs, start=1):
                print(f"{idx}) {job.name} (src={job.source}, dst={job.destination}, mode={job.mode})")
            sel = input("ë²ˆí˜¸ ì„ íƒ: ").strip()
            try:
                i = int(sel)
            except ValueError:
                continue
            if i == 0:
                job_name = None
            elif 1 <= i <= len(jobs):
                job_name = jobs[i - 1].name
            else:
                continue

            dry_run = input("Dry-run ëª¨ë“œë¡œ ì‹¤í–‰í• ê¹Œìš”? [y/N]: ").strip().lower() == 'y'
            resume = input("resume ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í• ê¹Œìš”? [y/N]: ").strip().lower() == 'y'

            args = SimpleNamespace(
                command="backup",
                config=str(config_path),
                job=job_name,
                dry_run=dry_run,
                log_dir=str(base_log_dir),
                resume=resume,
            )
            _run_backup(args, tui=None)
            input("ë°±ì—…ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. Enter ë¥¼ ëˆ„ë¥´ë©´ ë©”ì¸ ë©”ë‰´ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")

        elif choice == '2':
            latest = get_latest_journal(base_log_dir)
            if not latest:
                print("ìµœê·¼ ì €ë„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                input("ê³„ì†í•˜ë ¤ë©´ Enter...")
                continue
            j = load_journal(latest)
            print(f"ìµœê·¼ ì €ë„: {latest.name}")
            print(f"job={j.job_name}, ts={j.timestamp}, status={j.status}")
            yn = input("ì´ ì €ë„ë¡œ ë¡¤ë°±í• ê¹Œìš”? [y/N]: ").strip().lower()
            if yn == 'y':
                rollback_journal(j, dry_run=False)
                j.status = "rolled_back"
                save_journal(j, latest)
                print("ë¡¤ë°± ì™„ë£Œ.")
            else:
                print("ë¡¤ë°± ì·¨ì†Œ.")
            input("ê³„ì†í•˜ë ¤ë©´ Enter...")

        elif choice == '3':
            journals = sorted(base_log_dir.glob("journal_*.json"))
            if not journals:
                print("journal_*.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                for j in journals[-100:]:
                    data = load_journal(j)
                    print(f"{j.name} | job={data.job_name} | ts={data.timestamp} | status={data.status}")
            input("ê³„ì†í•˜ë ¤ë©´ Enter...")

        elif choice == '4':
            snapshots_root = base_log_dir / "snapshots"
            if not snapshots_root.exists():
                print("snapshots ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                input("ê³„ì†í•˜ë ¤ë©´ Enter...")
                continue
            for job_dir in sorted(snapshots_root.iterdir()):
                if not job_dir.is_dir():
                    continue
                index_file = job_dir / "index.json"
                if not index_file.exists():
                    print(f"[{job_dir.name}] index.json ì—†ìŒ")
                    continue
                with index_file.open("r", encoding="utf-8") as f:
                    index = json.load(f)
                print(f"=== Job: {job_dir.name} (snapshots: {len(index)}) ===")
                for entry in index[-20:]:
                    print(
                        f"  ts={entry.get('timestamp')} | file={entry.get('snapshot_file')} "
                        f"| count={entry.get('file_count')} | at={entry.get('generated_at')}"
                    )
            input("ê³„ì†í•˜ë ¤ë©´ Enter...")

        elif choice == '5':
            print("ê¸°ì¡´ config ìˆ˜ì •(E), ìƒˆ config ìƒì„±(N), ì·¨ì†Œ(Q)")
            sub = input("> ").strip().lower()
            if sub == 'q' or not sub:
                continue

            if sub == 'n':
                path_str = input("ìƒˆë¡œ ë§Œë“¤ config JSON ê²½ë¡œ (ì˜ˆ: ./configs/my_backup.json): ").strip()
                if not path_str:
                    continue
                config_path = Path(path_str).expanduser()
                jobs: List[BackupJob] = []
            else:
                configs = find_config_files()
                if not configs:
                    print("ìë™ ê²€ìƒ‰ëœ config ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    path_str = input("ì§ì ‘ ê²½ë¡œ ì…ë ¥ (ì·¨ì†Œ: ë¹ˆ ì¤„): ").strip()
                    if not path_str:
                        continue
                    config_path = Path(path_str).expanduser()
                else:
                    print("config ëª©ë¡:")
                    for idx, c in enumerate(configs, start=1):
                        print(f"{idx}) {c}")
                    sel = input("ë²ˆí˜¸ ì„ íƒ ë˜ëŠ” ì§ì ‘ ê²½ë¡œ ì…ë ¥: ").strip()
                    try:
                        idx = int(sel)
                        config_path = configs[idx - 1]
                    except (ValueError, IndexError):
                        config_path = Path(sel).expanduser()

                try:
                    jobs = load_config(config_path)
                except Exception as e:
                    print(f"config ì½ê¸° ì‹¤íŒ¨: {e}")
                    input("ê³„ì†í•˜ë ¤ë©´ Enter...")
                    continue

            if jobs:
                print("0) ìƒˆ Job ì¶”ê°€")
                for idx, j in enumerate(jobs, start=1):
                    print(f"{idx}) {j.name} (mode={j.mode}, src={j.source}, dst={j.destination})")
                sel = input("ìˆ˜ì •í•  Job ë²ˆí˜¸ ì„ íƒ (0=ìƒˆ Job): ").strip()
            else:
                print("í˜„ì¬ Job ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆ Job ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                sel = "0"

            if sel == "0":
                job = BackupJob(
                    name="",
                    source=Path("."),
                    destination=Path("."),
                    mode="safety_net",
                    exclude=[],
                    safety_net_days=30,
                    verify=False,
                )
                jobs.append(job)
                editing_job = job
            else:
                try:
                    idx = int(sel)
                    editing_job = jobs[idx - 1]
                except Exception:
                    print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                    input("ê³„ì†í•˜ë ¤ë©´ Enter...")
                    continue

            def edit_field(label: str, current: str) -> str:
                v = input(f"{label} (í˜„ì¬: {current}) ìƒˆ ê°’(Enter=ìœ ì§€): ").strip()
                return current if v == "" else v

            editing_job.name = edit_field("Job ì´ë¦„", editing_job.name or "(ë¹ˆ ê°’)")
            editing_job.source = Path(edit_field("Source ê²½ë¡œ", str(editing_job.source))).expanduser()
            editing_job.destination = Path(edit_field("Destination ê²½ë¡œ", str(editing_job.destination))).expanduser()

            mode_val = edit_field("Mode (clone/sync/safety_net)", editing_job.mode)
            if mode_val in ("clone", "sync", "safety_net"):
                editing_job.mode = mode_val

            excl_str_current = ", ".join(editing_job.exclude) if editing_job.exclude else ""
            excl_str = edit_field("Exclude íŒ¨í„´(ì‰¼í‘œ êµ¬ë¶„)", excl_str_current)
            if excl_str != "":
                editing_job.exclude = [x.strip() for x in excl_str.split(",") if x.strip()]

            days_str = edit_field("SafetyNet ë³´ê´€ì¼ ìˆ˜", str(editing_job.safety_net_days))
            if days_str:
                try:
                    editing_job.safety_net_days = int(days_str)
                except ValueError:
                    pass

            verify_str = input(f"í•´ì‹œ ê²€ì¦(verify) ì‚¬ìš©? (í˜„ì¬: {editing_job.verify}) [y/N]: ").strip().lower()
            if verify_str in ("y", "yes"):
                editing_job.verify = True
            elif verify_str in ("n", "no"):
                editing_job.verify = False

            raw = {
                "jobs": [
                    {
                        "name": j.name,
                        "source": str(j.source),
                        "destination": str(j.destination),
                        "mode": j.mode,
                        "exclude": j.exclude,
                        "safety_net_days": j.safety_net_days,
                        "verify": j.verify,
                    }
                    for j in jobs
                ]
            }
            try:
                config_path.parent.mkdir(parents=True, exist_ok=True)
                with config_path.open("w", encoding="utf-8") as f:
                    json.dump(raw, f, indent=2, ensure_ascii=False)
                print(f"Config ì €ì¥ ì™„ë£Œ: {config_path}")
            except Exception as e:
                print(f"Config ì €ì¥ ì‹¤íŒ¨: {e}")

            input("ê³„ì†í•˜ë ¤ë©´ Enter...")

        else:
            continue


# ================ main ì§„ì…ì  =================

def main() -> None:
    if len(sys.argv) == 1:
        if sys.stdout.isatty() and curses is not None:
            curses.wrapper(interactive_main_curses)
        else:
            interactive_main_plain()
        return

    args = parse_args()
    if args.command == "backup":
        main_backup(args)
    elif args.command == "rollback":
        main_rollback(args)
    else:
        print("ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì…ë‹ˆë‹¤.", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
